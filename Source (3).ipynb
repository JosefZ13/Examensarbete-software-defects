{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "950c053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.base import clone\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import ttest_rel\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from scipy.optimize import differential_evolution\n",
    "from IPython.display import display\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "DATA_DIR = os.path.join(\"..\", \"data\")\n",
    "\n",
    "\n",
    "\n",
    "# Funktion f√∂r att utv√§rdera en modell\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_proba_raw = model.decision_function(X_test)\n",
    "        y_proba = (y_proba_raw - y_proba_raw.min()) / (y_proba_raw.max() - y_proba_raw.min() + 1e-9)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"y_proba\": y_proba,\n",
    "    }\n",
    "def print_confusion_matrix(y_true, y_pred, title=\"Confusion matrix\"):\n",
    "    \"\"\"\n",
    "    Skriver ut confusion matrix i tabellform.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm,\n",
    "        index=[\"True 0 (clean)\", \"True 1 (defect)\"],\n",
    "        columns=[\"Pred 0\", \"Pred 1\"],\n",
    "    )\n",
    "    print(title)\n",
    "    display(cm_df)\n",
    "\n",
    "\n",
    "POSSIBLE_TARGETS_GENERAL = [\n",
    "    \"defects\", \"Defects\", \"defect\", \"bug\", \"bugs\", \"problems\", \"problem\",\n",
    "    \"class\", \"target\"\n",
    "]\n",
    "\n",
    "# dataset-specifika kandidater (vi vet lite extra om vissa)\n",
    "POSSIBLE_TARGETS_BY_DATASET = {\n",
    "    \"JM1\": [\"defects\", \"Defects\"],\n",
    "    \"KC1\": [\"defects\", \"Defects\"],\n",
    "    \"KC2\": [\"problems\", \"bug\", \"bugs\", \"defects\", \"Defects\"],\n",
    "    \"PC1\": [\"defects\", \"Defects\"],\n",
    "    \"CM1\": [\"defects\", \"Defects\"],\n",
    "}\n",
    "\n",
    "DATASETS = {\n",
    "    \"JM1\": {\"filename\": \"jm1.csv\", \"target\": \"defects\"},\n",
    "    \"KC1\": {\"filename\": \"kc1.csv\", \"target\": \"defects\"},\n",
    "    \"KC2\": {\"filename\": \"kc2.csv\", \"target\": \"defects\"},\n",
    "    \"PC1\": {\"filename\": \"pc1.csv\", \"target\": \"defects\"},\n",
    "    \"CM1\": {\"filename\": \"cm1.csv\", \"target\": \"defects\"},\n",
    "}\n",
    "\n",
    "def load_and_prepare_dataset(dataset_name):\n",
    "    \"\"\"\n",
    "    L√§ser in valt dataset, f√∂rs√∂ker hitta r√§tt target-kolumn,\n",
    "    delar i train/test och skalar features.\n",
    "    \"\"\"\n",
    "    info = DATASETS[dataset_name]\n",
    "    path = os.path.join(DATA_DIR, info[\"filename\"])\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # 1) v√§lj kandidatlista f√∂r target-namn\n",
    "    candidate_targets = POSSIBLE_TARGETS_BY_DATASET.get(\n",
    "        dataset_name,\n",
    "        POSSIBLE_TARGETS_GENERAL\n",
    "    )\n",
    "\n",
    "    # 2) hitta f√∂rsta kolumn som matchar en kandidat\n",
    "    target_col = None\n",
    "    for cand in candidate_targets:\n",
    "        if cand in df.columns:\n",
    "            target_col = cand\n",
    "            break\n",
    "\n",
    "    if target_col is None:\n",
    "        raise ValueError(\n",
    "            f\"Kunde inte hitta target-kolumn i {dataset_name}.\\n\"\n",
    "            f\"F√∂rs√∂kte med: {candidate_targets}\\n\"\n",
    "            f\"Filen har kolumner: {list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    print(f\"Anv√§nder target-kolumn '{target_col}' f√∂r dataset {dataset_name}.\")\n",
    "\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    # s√§kerst√§ll 0/1\n",
    "    if y.dtype == \"bool\":\n",
    "        y = y.astype(int)\n",
    "    elif y.dtype == \"object\":\n",
    "        y = y.astype(str).str.lower().map({\n",
    "            \"yes\": 1,\n",
    "            \"true\": 1,\n",
    "            \"defective\": 1,\n",
    "            \"bug\": 1,\n",
    "            \"bugs\": 1,\n",
    "            \"problem\": 1,\n",
    "            \"problems\": 1,\n",
    "            \"1\": 1,\n",
    "            \"0\": 0,\n",
    "        }).fillna(0).astype(int)\n",
    "\n",
    "    print(f\"{dataset_name}: shape={df.shape}\")\n",
    "    print(\"Klassf√∂rdelning (hela datan):\")\n",
    "    print(y.value_counts(), \"\\n\")\n",
    "# H√ÑR SKER TRAIN- TEST SPLIT REDAN I B√ñRJAN F√ñR ATT HA SAMMA KLASSF√ñRDELNING\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2,\n",
    "        stratify=y,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "# Z-SCORE NORMALISWEERING. Den r√§knar ut medelv√§rdet och standard avvikelsen p√• tr√§ningsdata \n",
    "# g√∂r att alla features hamnar p√• samma skala. P√• s√• s√§tt slipper modellen bli p√•verkad av olika skalor.\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    print(\"Train klassf√∂rdelning:\")\n",
    "    print(y_train.value_counts())\n",
    "    print(\"\\nTest klassf√∂rdelning:\")\n",
    "    print(y_test.value_counts(), \"\\n\")\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "# === Del 3.5: Feature selection med RFE ===\n",
    "\n",
    "def apply_rfe(base_model, X_train, y_train, X_test, n_features_to_select=12):\n",
    "    rfe_estimator = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    selector = RFE(\n",
    "        estimator=rfe_estimator,\n",
    "        n_features_to_select=n_features_to_select,\n",
    "        step=1\n",
    "    )\n",
    "\n",
    "    # Anpassa X_train/X_test (RFE)\n",
    "    X_train_rfe = selector.fit_transform(X_train, y_train)\n",
    "    X_test_rfe = selector.transform(X_test)\n",
    "\n",
    "    print(f\"RFE: beh√•ller {n_features_to_select} features av totalt {X_train.shape[1]}\")\n",
    "\n",
    "    # üî¥ M√ÖSTE vara indenterad inuti funktionen\n",
    "    return X_train_rfe, X_test_rfe, selector\n",
    "\n",
    "\n",
    "# === Del 4: Modeller (alla basmodeller) ===\n",
    "\n",
    "def get_base_models():\n",
    "    \"\"\"\n",
    "    Skapar alla modeller vi vill testa.\n",
    "    \"\"\"\n",
    "    log_reg = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    ann = MLPClassifier(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation=\"relu\",\n",
    "        max_iter=200,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    svc = SVC(\n",
    "        kernel=\"rbf\",\n",
    "        probability=True,   # beh√∂vs f√∂r AUC\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    voting = VotingClassifier(\n",
    "        estimators=[\n",
    "            (\"logreg\", log_reg),\n",
    "            (\"rf\", rf),\n",
    "            (\"xgb\", xgb),\n",
    "        ],\n",
    "        voting=\"soft\"  # anv√§nder sannolikheter\n",
    "    )\n",
    "\n",
    "    models = {\n",
    "        \"LogisticRegression\": log_reg,\n",
    "        \"RandomForest\": rf,\n",
    "        \"XGBoost\": xgb,\n",
    "        \"ANN\": ann,\n",
    "        \"SVC\": svc,\n",
    "        \"Voting\": voting,\n",
    "    }\n",
    "    return models\n",
    "\n",
    "\n",
    "# === Del 5: SMOTE-varianter ===\n",
    "#kollar hur m√•nga minoritet det finns och till√§mpar ratio\n",
    "def apply_basic_smote(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Standard-SMOTE med default-parametrar.\n",
    "    \"\"\"\n",
    "    smote = SMOTE(random_state=RANDOM_STATE)\n",
    "    X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "    print(\"Efter basic SMOTE:\")\n",
    "    print(pd.Series(y_res).value_counts(), \"\\n\")\n",
    "    return X_res, y_res\n",
    "\n",
    "\n",
    "def smote_grid_search(model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Enkel grid search p√• SMOTE-parametrar (inspirerad av SMOTUNED-id√©n).\n",
    "    \"\"\"\n",
    "    pipe = Pipeline([\n",
    "        (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
    "        (\"clf\", model),\n",
    "    ])\n",
    "\n",
    "    # OBS: dubbel underscore f√∂r pipeline-parametrar!\n",
    "    param_grid = {\n",
    "        \"smote__k_neighbors\": [3, 5, 7],\n",
    "        \"smote__sampling_strategy\": [0.5, 0.75, 1.0],\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        pipe,\n",
    "        param_grid,\n",
    "        scoring=\"f1\",\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(\"GRID-SMOTE ‚Äì b√§sta parametrar:\", grid.best_params_)\n",
    "    return grid.best_estimator_\n",
    "\n",
    "\n",
    "def smotuned_de(model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    F√∂renklad SMOTUNED-id√©:\n",
    "    differential evolution optimerar SMOTE-parametrar (k_neighbors, sampling_strategy)\n",
    "    f√∂r att maximera F1 med 3-fold CV.\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(params):\n",
    "        # params = [k_neighbors, sampling_strategy]\n",
    "        k = int(round(params[0]))\n",
    "        k = max(2, min(k, 15))   # h√•ll k inom [2, 15]\n",
    "\n",
    "        sampling = float(params[1])\n",
    "        sampling = max(0.2, min(sampling, 1.0))  # sampling_strategy inom [0.2, 1.0]\n",
    "\n",
    "        smote = SMOTE(\n",
    "            k_neighbors=k,\n",
    "            sampling_strategy=sampling,\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "\n",
    "        X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scores = []\n",
    "\n",
    "        for train_idx, val_idx in cv.split(X_res, y_res):\n",
    "            X_tr, X_val = X_res[train_idx], X_res[val_idx]\n",
    "            y_tr, y_val = y_res[train_idx], y_res[val_idx]\n",
    "\n",
    "            m = clone(model)\n",
    "            m.fit(X_tr, y_tr)\n",
    "            y_pred = m.predict(X_val)\n",
    "            scores.append(f1_score(y_val, y_pred, zero_division=0))\n",
    "\n",
    "        # differential_evolution minimerar, s√• vi returnerar -F1\n",
    "        return -np.mean(scores)\n",
    "\n",
    "    bounds = [\n",
    "        (2, 15),    # k_neighbors\n",
    "        (0.2, 1.0), # sampling_strategy\n",
    "    ]\n",
    "\n",
    "    result = differential_evolution(\n",
    "        objective,\n",
    "        bounds,\n",
    "        maxiter=15,\n",
    "        popsize=10,\n",
    "        tol=0.01,\n",
    "        polish=True,\n",
    "        disp=False,\n",
    "    )\n",
    "\n",
    "    best_k = int(round(result.x[0]))\n",
    "    best_sampling = float(result.x[1])\n",
    "    best_k = max(2, min(best_k, 15))\n",
    "    best_sampling = max(0.2, min(best_sampling, 1.0))\n",
    "\n",
    "    print(\"SMOTUNED-DE ‚Äì b√§sta parametrar:\")\n",
    "    print(\"k_neighbors:\", best_k)\n",
    "    print(\"sampling_strategy:\", best_sampling)\n",
    "\n",
    "    best_smote = SMOTE(\n",
    "        k_neighbors=best_k,\n",
    "        sampling_strategy=best_sampling,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "    X_res_best, y_res_best = best_smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    final_model = clone(model)\n",
    "    final_model.fit(X_res_best, y_res_best)\n",
    "\n",
    "    return final_model\n",
    "\n",
    "def train_with_smote_mode(base_model, X_train, y_train, smote_mode): # Hj√§lpfunktion som hanterar smote_mode\n",
    "    model = clone(base_model)\n",
    "    \n",
    "    if smote_mode == \"NONE\":\n",
    "        print(\"Ingen SMOTE anv√§nds.\\n\")\n",
    "        model.fit(X_train, y_train)\n",
    "        used_model = model\n",
    "        smote_label = \"NONE\"\n",
    "    elif smote_mode == \"BASIC\":\n",
    "        X_smote, y_smote = apply_basic_smote(X_train, y_train)\n",
    "        model.fit(X_smote, y_smote)\n",
    "        used_model = model\n",
    "        smote_label = \"BASIC\"\n",
    "\n",
    "    elif smote_mode == \"GRID\":\n",
    "        # grid-funktionen tr√§nar sj√§lv och returnerar b√§sta estimatorn\n",
    "        used_model = smote_grid_search(model, X_train, y_train)\n",
    "        smote_label = \"GRID\"\n",
    "\n",
    "    elif smote_mode == \"SMOTUNED-DE\":\n",
    "        # smotuned_de tr√§nar ocks√• och returnerar en f√§rdig modell\n",
    "        used_model = smotuned_de(model, X_train, y_train)\n",
    "        smote_label = \"SMOTUNED-DE\"\n",
    "\n",
    "    else:\n",
    "        print(\"Ogiltigt SMOTE-l√§ge, anv√§nder NONE.\")\n",
    "        model.fit(X_train, y_train)\n",
    "        used_model = model\n",
    "        smote_label = \"NONE\"\n",
    "\n",
    "    return used_model, smote_label    \n",
    "\n",
    "def cross_val_f1_scores(dataset_name, model_name, smote_mode=\"NONE\", use_rfe=False, n_features_to_select=12, n_splits=3):\n",
    "    \"\"\"\n",
    "    K√∂r StratifiedKFold CV och returnerar en lista med F1-scores f√∂r vald modell.\n",
    "    \"\"\"\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = load_and_prepare_dataset(dataset_name)\n",
    "    # Vi sl√•r ihop train+test h√§r f√∂r att g√∂ra CV p√• hela datasetet\n",
    "    X_all = np.vstack([X_train_scaled, X_test_scaled])\n",
    "    y_all = np.concatenate([y_train.values, y_test.values])\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    base_models = get_base_models()\n",
    "    base_model = base_models[model_name]\n",
    "\n",
    "    f1_scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_all, y_all), start=1):\n",
    "        X_tr, X_val = X_all[train_idx], X_all[val_idx]\n",
    "        y_tr, y_val = y_all[train_idx], y_all[val_idx]\n",
    "\n",
    "        # ev. RFE per fold\n",
    "        if use_rfe:\n",
    "            X_tr, X_val, _ = apply_rfe(\n",
    "                base_model=base_model,\n",
    "                X_train=X_tr,\n",
    "                y_train=y_tr,\n",
    "                X_test=X_val,\n",
    "                n_features_to_select=n_features_to_select,\n",
    "            )\n",
    "\n",
    "        used_model, smote_label = train_with_smote_mode(\n",
    "            base_model=base_model,\n",
    "            X_train=X_tr,\n",
    "            y_train=y_tr,\n",
    "            smote_mode=smote_mode,\n",
    "        )\n",
    "\n",
    "        y_pred = used_model.predict(X_val)\n",
    "        f1 = f1_score(y_val, y_pred, zero_division=0)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        print(f\"[{dataset_name}] Fold {fold_idx}: F1 = {f1:.4f} (SMOTE={smote_label})\")\n",
    "\n",
    "    return np.array(f1_scores)\n",
    "   \n",
    "# === Del 6: j√§mf√∂relsefunktion f√∂r EN modell + EN dataset ===\n",
    "\n",
    "def compare_smote_variants(dataset_name, model_name, use_rfe=False, n_features_to_select=12):\n",
    "    \"\"\"\n",
    "    K√∂r SAMMA dataset + SAMMA modell med:\n",
    "    - ingen SMOTE\n",
    "    - basic SMOTE\n",
    "    - GRID-SMOTE\n",
    "    - SMOTUNED-DE\n",
    "    och returnerar en tabell med nyckeltal + pivot p√• F1.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Ladda och skala data\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = load_and_prepare_dataset(dataset_name)\n",
    "\n",
    "    # 1b) Valfritt: Feature selection med RFE\n",
    "    if use_rfe:\n",
    "        print(f\"\\n>>> K√∂r RFE med {n_features_to_select} features f√∂r {dataset_name} / {model_name} <<<\\n\")\n",
    "        temp_models = get_base_models()\n",
    "        rfe_base_model = temp_models[model_name]\n",
    "\n",
    "        X_train_used, X_test_used, rfe_selector = apply_rfe(\n",
    "            base_model=rfe_base_model,\n",
    "            X_train=X_train_scaled,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test_scaled,\n",
    "            n_features_to_select=n_features_to_select,\n",
    "        )\n",
    "    else:\n",
    "        X_train_used = X_train_scaled\n",
    "        X_test_used = X_test_scaled\n",
    "\n",
    "    # 2) H√§mta vald basmodell\n",
    "    base_models = get_base_models()\n",
    "    if model_name not in base_models:\n",
    "        raise ValueError(f\"Modell '{model_name}' finns inte. Tillg√§ngliga: {list(base_models.keys())}\")\n",
    "    base_model = base_models[model_name]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # 3) Alla SMOTE-l√§gen vi vill j√§mf√∂ra\n",
    "    smote_modes = [\"NONE\", \"BASIC\", \"GRID\", \"SMOTUNED-DE\"]\n",
    "\n",
    "    for mode in smote_modes:\n",
    "        print(f\"\\n--- Tr√§nar {model_name} p√• {dataset_name} med SMOTE-l√§ge: {mode} ---\")\n",
    "\n",
    "        used_model, smote_label = train_with_smote_mode(\n",
    "            base_model=base_model,\n",
    "            X_train=X_train_used,   # <-- RFE-version eller original\n",
    "            y_train=y_train,\n",
    "            smote_mode=mode,\n",
    "        )\n",
    "\n",
    "        # Utv√§rdera modellen\n",
    "        res = evaluate_model(used_model, X_test_used, y_test)\n",
    "\n",
    "        # Visa metrics i klartext\n",
    "        print(f\"\\nResultat ‚Äì {dataset_name} ‚Äì {model_name} ‚Äì SMOTE={smote_label}\")\n",
    "        print(f\"Recall   : {res['recall']:.4f}\")\n",
    "        print(f\"Precision: {res['precision']:.4f}\")\n",
    "        print(f\"F1-score : {res['f1']:.4f}\")\n",
    "        print(f\"Accuracy : {res['accuracy']:.4f}\")\n",
    "        print(f\"AUC      : {res['auc']:.4f}\")\n",
    "\n",
    "        # Confusion matrix EFTER scoren\n",
    "        print_confusion_matrix(y_test, res[\"y_pred\"], title=f\"Confusion matrix ({dataset_name}, {model_name}, SMOTE={smote_label})\")\n",
    "\n",
    "        # Spara till tabell\n",
    "        results.append({\n",
    "            \"dataset\": dataset_name,\n",
    "            \"model\": model_name,\n",
    "            \"smote_mode\": smote_label,\n",
    "            \"accuracy\": res[\"accuracy\"],\n",
    "            \"precision\": res[\"precision\"],\n",
    "            \"recall\": res[\"recall\"],\n",
    "            \"f1\": res[\"f1\"],\n",
    "            \"auc\": res[\"auc\"],\n",
    "        })\n",
    "\n",
    "    # 4) L√§gg allt i en DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    print(f\"\\n=== J√§mf√∂relse SMOTE-varianter ‚Äì dataset: {dataset_name}, modell: {model_name} ===\")\n",
    "    display(df)\n",
    "\n",
    "    # 5) Pivot-tabell p√• F1 (som i dina tabeller)\n",
    "    pivot_f1 = df.pivot_table(\n",
    "        index=[\"dataset\", \"model\"],\n",
    "        columns=\"smote_mode\",\n",
    "        values=\"f1\"\n",
    "    )\n",
    "    print(\"\\nF1 per SMOTE-l√§ge:\")\n",
    "    display(pivot_f1)\n",
    "        \n",
    "    return df, pivot_f1\n",
    "\n",
    "\n",
    "def cross_project_experiment(\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    model_name,\n",
    "    smote_mode=\"SMOTUNED-DE\",\n",
    "    use_rfe=False,\n",
    "    n_features_to_select=12,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train on one dataset (train_dataset) and test on another (test_dataset).\n",
    "    - Skalar features baserat p√• TRAIN och anv√§nder samma transformation p√• TEST.\n",
    "    - Alignar features: beh√•ller bara gemensamma kolumner med samma namn.\n",
    "    - Kan anv√§nda SMOTE-l√§ge + ev. RFE (use_rfe=True).\n",
    "    \"\"\"\n",
    "    # ===== 1) L√§s in TRAIN-data =====\n",
    "    info_train = DATASETS[train_dataset]\n",
    "    path_train = os.path.join(DATA_DIR, info_train[\"filename\"])\n",
    "    df_train = pd.read_csv(path_train)\n",
    "\n",
    "    candidate_targets_train = POSSIBLE_TARGETS_BY_DATASET.get(\n",
    "        train_dataset,\n",
    "        POSSIBLE_TARGETS_GENERAL\n",
    "    )\n",
    "\n",
    "    target_col_train = None\n",
    "    for cand in candidate_targets_train:\n",
    "        if cand in df_train.columns:\n",
    "            target_col_train = cand\n",
    "            break\n",
    "\n",
    "    if target_col_train is None:\n",
    "        raise ValueError(f\"Hittade ingen target-kolumn i {train_dataset}\")\n",
    "\n",
    "    X_train = df_train.drop(columns=[target_col_train])\n",
    "    y_train = df_train[target_col_train]\n",
    "\n",
    "    if y_train.dtype == \"bool\":\n",
    "        y_train = y_train.astype(int)\n",
    "    elif y_train.dtype == \"object\":\n",
    "        y_train = y_train.astype(str).str.lower().map({\n",
    "            \"yes\": 1, \"true\": 1, \"defective\": 1, \"bug\": 1, \"bugs\": 1,\n",
    "            \"problem\": 1, \"problems\": 1, \"1\": 1, \"0\": 0,\n",
    "        }).fillna(0).astype(int)\n",
    "\n",
    "    # ===== 2) L√§s in TEST-data =====\n",
    "    info_test = DATASETS[test_dataset]\n",
    "    path_test = os.path.join(DATA_DIR, info_test[\"filename\"])\n",
    "    df_test = pd.read_csv(path_test)\n",
    "\n",
    "    candidate_targets_test = POSSIBLE_TARGETS_BY_DATASET.get(\n",
    "        test_dataset,\n",
    "        POSSIBLE_TARGETS_GENERAL\n",
    "    )\n",
    "\n",
    "    target_col_test = None\n",
    "    for cand in candidate_targets_test:\n",
    "        if cand in df_test.columns:\n",
    "            target_col_test = cand\n",
    "            break\n",
    "\n",
    "    if target_col_test is None:\n",
    "        raise ValueError(f\"Hittade ingen target-kolumn i {test_dataset}\")\n",
    "\n",
    "    X_test = df_test.drop(columns=[target_col_test])\n",
    "    y_test = df_test[target_col_test]\n",
    "\n",
    "    if y_test.dtype == \"bool\":\n",
    "        y_test = y_test.astype(int)\n",
    "    elif y_test.dtype == \"object\":\n",
    "        y_test = y_test.astype(str).str.lower().map({\n",
    "            \"yes\": 1, \"true\": 1, \"defective\": 1, \"bug\": 1, \"bugs\": 1,\n",
    "            \"problem\": 1, \"problems\": 1, \"1\": 1, \"0\": 0,\n",
    "        }).fillna(0).astype(int)\n",
    "\n",
    "    #print(f\"\\n=== {train_dataset} ‚Üí {test_dataset} ===\")\n",
    "    #print(\"Train klassf√∂rdelning:\")\n",
    "    #print(y_train.value_counts())\n",
    "    #print(\"\\nTest klassf√∂rdelning:\")\n",
    "    #print(y_test.value_counts(), \"\\n\")\n",
    "\n",
    "    # ===== 3) Aligna features: beh√•ll bara gemensamma kolumner =====\n",
    "    common_features = sorted(set(X_train.columns) & set(X_test.columns))\n",
    "\n",
    "    if len(common_features) == 0:\n",
    "        raise ValueError(\"Inga gemensamma features mellan train och test!\")\n",
    "\n",
    "    if len(common_features) < X_train.shape[1] or len(common_features) < X_test.shape[1]:\n",
    "        dropped_train = set(X_train.columns) - set(common_features)\n",
    "        dropped_test = set(X_test.columns) - set(common_features)\n",
    "        print(f\"Gemensamma features: {len(common_features)}\")\n",
    "        if dropped_train:\n",
    "            print(\"Features som bara fanns i TRAIN och togs bort:\", dropped_train)\n",
    "        if dropped_test:\n",
    "            print(\"Features som bara fanns i TEST och togs bort:\", dropped_test)\n",
    "        print()\n",
    "\n",
    "    X_train = X_train[common_features].copy()\n",
    "    X_test = X_test[common_features].copy()\n",
    "\n",
    "    # ===== 4) Skala =====\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # ===== 5) Ev. RFE =====\n",
    "    if use_rfe:\n",
    "        print(f\">>> Cross-project RFE: beh√•ller {n_features_to_select} features <<<\")\n",
    "        temp_models = get_base_models()\n",
    "        rfe_base_model = temp_models[model_name]\n",
    "\n",
    "        X_train_scaled, X_test_scaled, rfe_selector = apply_rfe(\n",
    "            base_model=rfe_base_model,\n",
    "            X_train=X_train_scaled,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test_scaled,\n",
    "            n_features_to_select=n_features_to_select,\n",
    "        )\n",
    "\n",
    "    # ===== 6) Modell + SMOTE-l√§ge =====\n",
    "    base_models = get_base_models()\n",
    "    base_model = base_models[model_name]\n",
    "\n",
    "    used_model, smote_label = train_with_smote_mode(\n",
    "        base_model=base_model,\n",
    "        X_train=X_train_scaled,\n",
    "        y_train=y_train,\n",
    "        smote_mode=smote_mode,\n",
    "    )\n",
    "\n",
    "    # ===== 7) Utv√§rdera p√• TEST =====\n",
    "    res = evaluate_model(used_model, X_test_scaled, y_test)\n",
    "\n",
    "    print(f\"Resultat ‚Äì {train_dataset} ‚Üí {test_dataset} ‚Äì {model_name} ‚Äì SMOTE: {smote_label}\")\n",
    "    for k, v in res.items():\n",
    "        if k in [\"y_pred\", \"y_proba\"]:\n",
    "            continue\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "    return res\n",
    "def build_cpdp_table(train_dataset, test_dataset, model_name, n_features_to_select=12):\n",
    "    \"\"\"\n",
    "    Bygger en tabell med metrics f√∂r tre CPDP-inst√§llningar:\n",
    "    - Baseline (ingen SMOTE, ingen RFE)\n",
    "    - SMOTUNED-DE (utan RFE)\n",
    "    - SMOTUNED-DE + RFE (feature selection, t.ex. 12 features)\n",
    "\n",
    "    Anv√§nder cross_project_experiment(...) under huven.\n",
    "    \"\"\"\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    settings = [\n",
    "        (\"Baseline (NONE)\",        \"NONE\",        False),\n",
    "        (\"SMOTUNED-DE\",            \"SMOTUNED-DE\", False),\n",
    "        (\"SMOTUNED-DE + RFE(12)\",  \"SMOTUNED-DE\", True),\n",
    "    ]\n",
    "\n",
    "    for setting_name, smote_mode, use_rfe in settings:\n",
    "        print(f\"\\n>>> K√∂r CPDP: {train_dataset} ‚Üí {test_dataset}, \"\n",
    "              f\"modell={model_name}, setting={setting_name} (SMOTE={smote_mode}, RFE={use_rfe})\")\n",
    "\n",
    "        res = cross_project_experiment(\n",
    "            train_dataset=train_dataset,\n",
    "            test_dataset=test_dataset,\n",
    "            model_name=model_name,\n",
    "            smote_mode=smote_mode,\n",
    "            use_rfe=use_rfe,\n",
    "            n_features_to_select=n_features_to_select,\n",
    "        )\n",
    "\n",
    "        rows.append({\n",
    "            \"Train ‚Üí Test\": f\"{train_dataset} ‚Üí {test_dataset}\",\n",
    "            \"Model\":        model_name,\n",
    "            \"Setting\":      setting_name,\n",
    "            \"Recall\":       res[\"recall\"],\n",
    "            \"Precision\":    res[\"precision\"],\n",
    "            \"F1\":           res[\"f1\"],\n",
    "            \"Accuracy\":     res[\"accuracy\"],\n",
    "            \"AUC\":          res[\"auc\"],\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "\n",
    "# === Del 7: Meny f√∂r att k√∂ra experiment ===\n",
    "\n",
    "def run_experiments_menu():\n",
    "    # v√§lj dataset\n",
    "    print(\"Tillg√§ngliga dataset:\")\n",
    "    dataset_names = list(DATASETS.keys())\n",
    "    for idx, name in enumerate(dataset_names, start=1):\n",
    "        print(f\"{idx} = {name}\")\n",
    "    print(\"ALL = alla dataset\")\n",
    "\n",
    "    dataset_choice = input(\"V√§lj dataset (t.ex. 1, 2, 3 eller JM1/KC1/ALL): \").strip().upper()\n",
    "\n",
    "    all_datasets_selected = False\n",
    "\n",
    "    if dataset_choice == \"ALL\":\n",
    "        datasets_to_run = dataset_names\n",
    "        all_datasets_selected = True\n",
    "    elif dataset_choice.isdigit():\n",
    "        idx = int(dataset_choice) - 1\n",
    "        if 0 <= idx < len(dataset_names):\n",
    "            datasets_to_run = [dataset_names[idx]]\n",
    "        else:\n",
    "            print(\"Ogiltigt sifferval, anv√§nder f√∂rsta datasetet.\")\n",
    "            datasets_to_run = [dataset_names[0]]\n",
    "    else:\n",
    "        # anta att anv√§ndaren skrev namnet direkt, t.ex. JM1\n",
    "        if dataset_choice in DATASETS:\n",
    "            datasets_to_run = [dataset_choice]\n",
    "        else:\n",
    "            print(\"Ogiltigt namn, anv√§nder f√∂rsta datasetet.\")\n",
    "            datasets_to_run = [dataset_names[0]]\n",
    "\n",
    "    # v√§lj modell(er)\n",
    "    models = get_base_models()\n",
    "    print(\"\\nTillg√§ngliga modeller:\")\n",
    "    model_names = list(models.keys())\n",
    "    for idx, name in enumerate(model_names, start=1):\n",
    "        print(f\"{idx} = {name}\")\n",
    "    print(\"ALL = alla modeller\")\n",
    "\n",
    "    model_choice = input(\"V√§lj modell (t.ex. 1, 2 eller RandomForest/ALL): \").strip()\n",
    "\n",
    "    all_models_selected = False\n",
    "\n",
    "    if model_choice.upper() == \"ALL\":\n",
    "        model_names_to_run = model_names\n",
    "        all_models_selected = True\n",
    "    elif model_choice.isdigit():\n",
    "        idx = int(model_choice) - 1\n",
    "        if 0 <= idx < len(model_names):\n",
    "            model_names_to_run = [model_names[idx]]\n",
    "        else:\n",
    "            print(\"Ogiltigt sifferval, anv√§nder f√∂rsta modellen.\")\n",
    "            model_names_to_run = [model_names[0]]\n",
    "    else:\n",
    "        if model_choice in models:\n",
    "            model_names_to_run = [model_choice]\n",
    "        else:\n",
    "            print(\"Ogiltigt modellnamn, anv√§nder f√∂rsta modellen.\")\n",
    "            model_names_to_run = [model_names[0]]\n",
    "\n",
    "    # v√§lj SMOTE-l√§ge\n",
    "    print(\"\\nSMOTE-l√§gen:\")\n",
    "    print(\"0 = Ingen SMOTE\")\n",
    "    print(\"1 = Basic SMOTE (standardparametrar)\")\n",
    "    print(\"2 = GRID-SMOTE (enkel tuning)\")\n",
    "    print(\"3 = SMOTUNED-DE (evolution√§r tuning)\")\n",
    "    print(\"4 = J√§mf√∂r ALLA SMOTE-varianter f√∂r vald dataset + modell\")\n",
    "    smote_mode = input(\"V√§lj 0 / 1 / 2 / 3 / 4: \").strip()\n",
    "\n",
    "    # üî∏ Specialfall: smote_mode 4 = k√∂r compare_smote_variants f√∂r EN kombination\n",
    "    if smote_mode == \"4\":\n",
    "        if len(datasets_to_run) == 1 and len(model_names_to_run) == 1:\n",
    "            ds = datasets_to_run[0]\n",
    "            mn = model_names_to_run[0]\n",
    "            df_compare, pivot_compare = compare_smote_variants(ds, mn)\n",
    "            return df_compare\n",
    "        else:\n",
    "            print(\"\\n‚ö† SMOTE-l√§ge 4 kr√§ver att du v√§ljer EXAKT ett dataset och en modell (inte ALL).\")\n",
    "            print(\"Byter till l√§ge 1 (Basic SMOTE) ist√§llet.\\n\")\n",
    "            smote_mode = \"1\"\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for ds in datasets_to_run:\n",
    "        print(\"\\n==============================\")\n",
    "        print(f\"K√∂r dataset: {ds}\")\n",
    "        print(\"==============================\\n\")\n",
    "\n",
    "        X_train_scaled, X_test_scaled, y_train, y_test = load_and_prepare_dataset(ds)\n",
    "\n",
    "        for model_name in model_names_to_run:\n",
    "            base_models = get_base_models()  # nya instanser\n",
    "            model = base_models[model_name]\n",
    "\n",
    "            print(f\"\\n--- Modell: {model_name} ---\")\n",
    "\n",
    "            # v√§lj tr√§ningsstrategi beroende p√• smote_mode\n",
    "            if smote_mode == \"0\":\n",
    "                print(\"Ingen SMOTE anv√§nds.\\n\")\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                used_model = model\n",
    "                smote_label = \"NONE\"\n",
    "\n",
    "            elif smote_mode == \"1\":\n",
    "                X_train_smote, y_train_smote = apply_basic_smote(X_train_scaled, y_train)\n",
    "                model.fit(X_train_smote, y_train_smote)\n",
    "                used_model = model\n",
    "                smote_label = \"BASIC\"\n",
    "\n",
    "            elif smote_mode == \"2\":\n",
    "                used_model = smote_grid_search(model, X_train_scaled, y_train)\n",
    "                smote_label = \"GRID\"\n",
    "\n",
    "            elif smote_mode == \"3\":\n",
    "                used_model = smotuned_de(model, X_train_scaled, y_train)\n",
    "                smote_label = \"SMOTUNED-DE\"\n",
    "\n",
    "            else:\n",
    "                print(\"Ogiltigt SMOTE-val, anv√§nder ingen SMOTE.\")\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                used_model = model\n",
    "                smote_label = \"NONE\"\n",
    "\n",
    "            # utv√§rdera\n",
    "            eval_results = evaluate_model(used_model, X_test_scaled, y_test)\n",
    "            print(f\"Resultat ‚Äì {ds} ‚Äì {model_name} ‚Äì SMOTE-l√§ge {smote_label}\")\n",
    "            for k, v in eval_results.items():\n",
    "                if k in [\"y_pred\", \"y_proba\"]:\n",
    "                    continue\n",
    "\n",
    "            all_results.append({\n",
    "                \"dataset\": ds,\n",
    "                \"model\": model_name,\n",
    "                \"smote_mode\": smote_label,\n",
    "                \"accuracy\": eval_results[\"accuracy\"],\n",
    "                \"precision\": eval_results[\"precision\"],\n",
    "                \"recall\": eval_results[\"recall\"],\n",
    "                \"f1\": eval_results[\"f1\"],\n",
    "                \"auc\": eval_results[\"auc\"],\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(\"\\n=== Sammanfattning av alla k√∂rningar ===\")\n",
    "    display(results_df)\n",
    "\n",
    "    # fortfarande: om du k√∂r ALL + ALL kan pivot-tabell vara nice\n",
    "    if all_datasets_selected and all_models_selected and not results_df.empty:\n",
    "        pivot_f1 = results_df.pivot_table(\n",
    "            index=[\"dataset\", \"model\"],\n",
    "            columns=\"smote_mode\",\n",
    "            values=\"f1\"\n",
    "        )\n",
    "        print(\"\\n=== F1 per dataset/modell och SMOTE-l√§ge ===\")\n",
    "        display(pivot_f1)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# k√∂r menyn\n",
    "#results_df = run_experiments_menu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7fe015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemensamma features: 20\n",
      "Features som bara fanns i TRAIN och togs bort: {'locCodeAndComment'}\n",
      "Features som bara fanns i TEST och togs bort: {'lOCodeAndComment'}\n",
      "\n",
      "Ingen SMOTE anv√§nds.\n",
      "\n",
      "Resultat ‚Äì KC1 ‚Üí KC2 ‚Äì RandomForest ‚Äì SMOTE: NONE\n",
      "accuracy: 0.7969\n",
      "precision: 0.5085\n",
      "recall: 0.2804\n",
      "f1: 0.3614\n",
      "auc: 0.7682\n",
      "\n",
      "=== Tabell: KC1 ‚Üí KC2 ‚Äì Baseline (NONE) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train ‚Üí Test</th>\n",
       "      <th>Model</th>\n",
       "      <th>Setting</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KC1 ‚Üí KC2</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>Baseline (NONE)</td>\n",
       "      <td>0.280374</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.361446</td>\n",
       "      <td>0.796935</td>\n",
       "      <td>0.768236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Train ‚Üí Test         Model          Setting    Recall  Precision        F1  \\\n",
       "0    KC1 ‚Üí KC2  RandomForest  Baseline (NONE)  0.280374   0.508475  0.361446   \n",
       "\n",
       "   Accuracy       AUC  \n",
       "0  0.796935  0.768236  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemensamma features: 20\n",
      "Features som bara fanns i TRAIN och togs bort: {'locCodeAndComment'}\n",
      "Features som bara fanns i TEST och togs bort: {'lOCodeAndComment'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_project_experiment\n",
    "\n",
    "# === CPDP: KC1 ‚Üí KC2 med RandomForest och olika SMOTE/RFE-l√§gen ===\n",
    "\n",
    "# 1) Baseline: ingen SMOTE, ingen RFE\n",
    "res_none = cross_project_experiment(\n",
    "    train_dataset=\"KC1\",\n",
    "    test_dataset=\"KC2\",\n",
    "    model_name=\"RandomForest\",\n",
    "    smote_mode=\"NONE\",\n",
    "    use_rfe=False\n",
    ")\n",
    "\n",
    "df_none = pd.DataFrame([{\n",
    "    \"Train ‚Üí Test\": \"KC1 ‚Üí KC2\",\n",
    "    \"Model\": \"RandomForest\",\n",
    "    \"Setting\": \"Baseline (NONE)\",\n",
    "    \"Recall\": res_none[\"recall\"],\n",
    "    \"Precision\": res_none[\"precision\"],\n",
    "    \"F1\": res_none[\"f1\"],\n",
    "    \"Accuracy\": res_none[\"accuracy\"],\n",
    "    \"AUC\": res_none[\"auc\"],\n",
    "}])\n",
    "\n",
    "print(\"\\n=== Tabell: KC1 ‚Üí KC2 ‚Äì Baseline (NONE) ===\")\n",
    "display(df_none)\n",
    "\n",
    "\n",
    "# 2) SMOTUNED-DE utan RFE\n",
    "res_smotuned = cross_project_experiment(\n",
    "    train_dataset=\"KC1\",\n",
    "    test_dataset=\"KC2\",\n",
    "    model_name=\"RandomForest\",\n",
    "    smote_mode=\"SMOTUNED-DE\",\n",
    "    use_rfe=False\n",
    ")\n",
    "\n",
    "df_smotuned = pd.DataFrame([{\n",
    "    \"Train ‚Üí Test\": \"KC1 ‚Üí KC2\",\n",
    "    \"Model\": \"RandomForest\",\n",
    "    \"Setting\": \"SMOTUNED-DE\",\n",
    "    \"Recall\": res_smotuned[\"recall\"],\n",
    "    \"Precision\": res_smotuned[\"precision\"],\n",
    "    \"F1\": res_smotuned[\"f1\"],\n",
    "    \"Accuracy\": res_smotuned[\"accuracy\"],\n",
    "    \"AUC\": res_smotuned[\"auc\"],\n",
    "}])\n",
    "\n",
    "print(\"\\n=== Tabell: KC1 ‚Üí KC2 ‚Äì SMOTUNED-DE (utan RFE) ===\")\n",
    "display(df_smotuned)\n",
    "\n",
    "\n",
    "# 3) SMOTUNED-DE + RFE (12 features)\n",
    "res_smotuned_rfe = cross_project_experiment(\n",
    "    train_dataset=\"KC1\",\n",
    "    test_dataset=\"KC2\",\n",
    "    model_name=\"RandomForest\",\n",
    "    smote_mode=\"SMOTUNED-DE\",\n",
    "    use_rfe=True,\n",
    "    n_features_to_select=12\n",
    ")\n",
    "\n",
    "df_smotuned_rfe = pd.DataFrame([{\n",
    "    \"Train ‚Üí Test\": \"KC1 ‚Üí KC2\",\n",
    "    \"Model\": \"RandomForest\",\n",
    "    \"Setting\": \"SMOTUNED-DE + RFE(12)\",\n",
    "    \"Recall\": res_smotuned_rfe[\"recall\"],\n",
    "    \"Precision\": res_smotuned_rfe[\"precision\"],\n",
    "    \"F1\": res_smotuned_rfe[\"f1\"],\n",
    "    \"Accuracy\": res_smotuned_rfe[\"accuracy\"],\n",
    "    \"AUC\": res_smotuned_rfe[\"auc\"],\n",
    "}])\n",
    "\n",
    "print(\"\\n=== Tabell: KC1 ‚Üí KC2 ‚Äì SMOTUNED-DE + RFE(12) ===\")\n",
    "display(df_smotuned_rfe)\n",
    "\n",
    "\n",
    "# (Extra) Kombinera alla tre i EN tabell f√∂r rapporten\n",
    "df_kc1_kc2_all = pd.concat([df_none, df_smotuned, df_smotuned_rfe], ignore_index=True)\n",
    "metrics = [\"Recall\", \"Precision\", \"F1\", \"Accuracy\", \"AUC\"]\n",
    "df_kc1_kc2_all[metrics] = df_kc1_kc2_all[metrics].round(4)\n",
    "\n",
    "print(\"\\n=== Samlad tabell ‚Äì KC1 ‚Üí KC2, RandomForest, tre inst√§llningar ===\")\n",
    "display(df_kc1_kc2_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a4c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    dataset_name: str,\n",
    "    model_name: str,\n",
    "    smote_mode: str = \"NONE\",\n",
    "    use_rfe: bool = False,\n",
    "    n_features_to_select: int = 12\n",
    "):\n",
    "    \"\"\"\n",
    "    K√∂r ETT experiment p√• ETT dataset med EN modell.\n",
    "\n",
    "    - dataset_name: t.ex. \"JM1\", \"KC1\", \"KC2\"\n",
    "    - model_name:   t.ex. \"LogisticRegression\", \"SVC\", \"RandomForest\", \"XGBoost\", \"Voting\"\n",
    "    - smote_mode:   \"NONE\", \"BASIC\", \"GRID\", \"SMOTUNED-DE\"\n",
    "    - use_rfe:      True/False (om du vill k√∂ra RFE feature selection)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Ladda & skala data\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = load_and_prepare_dataset(dataset_name)\n",
    "\n",
    "    X_train_used = X_train_scaled\n",
    "    X_test_used = X_test_scaled\n",
    "\n",
    "    # 2) Ev. RFE\n",
    "    if use_rfe:\n",
    "        print(f\">>> K√∂r RFE ({n_features_to_select} features) f√∂r {dataset_name} / {model_name} <<<\")\n",
    "        temp_models = get_base_models()\n",
    "        rfe_base_model = temp_models[model_name]\n",
    "\n",
    "        X_train_used, X_test_used, rfe_selector = apply_rfe(\n",
    "            base_model=rfe_base_model,\n",
    "            X_train=X_train_scaled,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test_scaled,\n",
    "            n_features_to_select=n_features_to_select,\n",
    "        )\n",
    "\n",
    "    # 3) H√§mta basmodell\n",
    "    base_models = get_base_models()\n",
    "    if model_name not in base_models:\n",
    "        raise ValueError(f\"Modell '{model_name}' finns inte. Tillg√§ngliga: {list(base_models.keys())}\")\n",
    "    base_model = base_models[model_name]\n",
    "\n",
    "    # 4) Tr√§na med valt SMOTE-l√§ge\n",
    "    used_model, smote_label = train_with_smote_mode(\n",
    "        base_model=base_model,\n",
    "        X_train=X_train_used,\n",
    "        y_train=y_train,\n",
    "        smote_mode=smote_mode,\n",
    "    )\n",
    "\n",
    "    # 5) Utv√§rdera p√• testdata\n",
    "    res = evaluate_model(used_model, X_test_used, y_test)\n",
    "\n",
    "    print(f\"\\nResultat ‚Äì {dataset_name} ‚Äì {model_name} ‚Äì SMOTE={smote_label} ‚Äì RFE={use_rfe}\")\n",
    "    print(f\"Recall   : {res['recall']:.4f}\")\n",
    "    print(f\"Precision: {res['precision']:.4f}\")\n",
    "    print(f\"F1-score : {res['f1']:.4f}\")\n",
    "    print(f\"Accuracy : {res['accuracy']:.4f}\")\n",
    "    print(f\"AUC      : {res['auc']:.4f}\")\n",
    "\n",
    "    return res[\"recall\"], res[\"precision\"], res[\"f1\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b82066",
   "metadata": {},
   "source": [
    "K√ñRINGSFUNKTIONER ANV√ÑND SAMMA MEN BYT UT MODELL OCH DATASET\n",
    "Starta exprimentet med hj√§lpfunktionen ovan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26a5cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LR + SVM -ingen SMOTE, ingen RFE Tabell 4.1\n",
    "\n",
    "recall_lr_ns, prec_lr_ns, f1_lr_ns = run_experiment(\n",
    "    dataset_name=\"JM1\",\n",
    "    model_name=\"LogisticRegression\",\n",
    "    smote_mode=\"NONE\",\n",
    "    use_rfe=False\n",
    ")\n",
    "\n",
    "recall_svm_ns, prec_svm_ns, f1_svm_ns = run_experiment(\n",
    "    dataset_name=\"JM1\",\n",
    "    model_name=\"SVC\",\n",
    "    smote_mode=\"NONE\",\n",
    "    use_rfe=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8ba3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Logistic Regression & SVM ‚Äì BASIC SMOTE, NO RFE (JM1) Tabell 4.2\n",
    "\n",
    "recall_lr_sm, prec_lr_sm, f1_lr_sm = run_experiment(\n",
    "    dataset_name=\"JM1\",\n",
    "    model_name=\"LogisticRegression\",\n",
    "    smote_mode=\"BASIC\",   # vanlig SMOTE\n",
    "    use_rfe=False\n",
    ")\n",
    "\n",
    "recall_svm_sm, prec_svm_sm, f1_svm_sm = run_experiment(\n",
    "    dataset_name=\"JM1\",\n",
    "    model_name=\"SVC\",\n",
    "    smote_mode=\"BASIC\",   # vanlig SMOTE\n",
    "    use_rfe=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca86a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. RandomForest, XGBoost, Voting ‚Äì NO SMOTE, NO RFE (JM1) Tabell4.3\n",
    "#RF,XGBoost, Voting - ingen SMOTE, ingen RFE\n",
    "\n",
    "recall_rf_ns, prec_rf_ns, f1_rf_ns = run_experiment(\n",
    "    dataset_name=\"JM1\",\n",
    "    model_name=\"RandomForest\",\n",
    "    smote_mode=\"NONE\",\n",
    "    use_rfe=False\n",
    ")\n",
    "\n",
    "recall_xgb_ns, prec_xgb_ns, f1_xgb_ns = run_experiment(\n",
    "    dataset_name=\"JM1\",\n",
    "    model_name=\"XGBoost\",\n",
    "    smote_mode=\"NONE\",\n",
    "    use_rfe=False\n",
    ")\n",
    "\n",
    "recall_vot_ns, prec_vot_ns, f1_vot_ns = run_experiment(\n",
    "    dataset_name=\"JM1\",\n",
    "    model_name=\"Voting\",\n",
    "    smote_mode=\"NONE\",\n",
    "    use_rfe=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee0dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. RandomForest, XGBoost, Voting ‚Äì NO SMOTE, WITH RFE (JM1)\n",
    "# N√§r RFE anv√§nds, beh√•ll 12 features (enligt texten) Tabell 4.4 vi anv√§nder Recrusive Feature Eliminaton\n",
    "#den rangordnar features baserat p√• deras betydelse f√∂r en RandomForest-modell och v√§ljer de 12 mest betydelsefulla.\n",
    "# Masken v√§ljer vilka features som modellen ska tr√§nas p√•\n",
    "recall_rf_fs, prec_rf_fs, f1_rf_fs = run_experiment(\n",
    "    dataset_name=\"JM1\",\n",
    "    model_name=\"RandomForest\",\n",
    "    smote_mode=\"NONE\",\n",
    "    use_rfe=True,\n",
    "    n_features_to_select=12\n",
    ")\n",
    "\n",
    "recall_xgb_fs, prec_xgb_fs, f1_xgb_fs = run_experiment(\n",
    "    dataset_name=\"JM1\",\n",
    "    model_name=\"XGBoost\",\n",
    "    smote_mode=\"NONE\",\n",
    "    use_rfe=True,\n",
    "    n_features_to_select=12\n",
    ")\n",
    "\n",
    "recall_vot_fs, prec_vot_fs, f1_vot_fs = run_experiment(\n",
    "    dataset_name=\"JM1\",\n",
    "    model_name=\"Voting\",\n",
    "    smote_mode=\"NONE\",\n",
    "    use_rfe=True,\n",
    "    n_features_to_select=12\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c8f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. RandomForest, XGBoost, Votingcommand:workbench.action.openLargeOutput?4f46e60f-23e8-4021-ace8-cf139781a49f ‚Äì SMOTUNED-DE, WITH RFE (JM1)\n",
    "_#Rf, Xgboost, Voting - Smote +RFE tabell 4.5\n",
    "recall_rf_smfs, prec_rf_smfs, f1_rf_smfs = run_experiment(\n",
    "    dataset_name=\"JM1\",\n",
    "    model_name=\"RandomForest\",\n",
    "    smote_mode=\"SMOTUNED-DE\",\n",
    "    use_rfe=True,\n",
    "    n_features_to_select=12\n",
    ")\n",
    "\n",
    "recall_xgb_smfs, prec_xgb_smfs, f1_xgb_smfs = run_experiment(\n",
    "    dataset_name=\"JM1\",\n",
    "    model_name=\"XGBoost\",\n",
    "    smote_mode=\"SMOTUNED-DE\",\n",
    "    use_rfe=True,\n",
    "    n_features_to_select=12\n",
    ")\n",
    "\n",
    "recall_vot_smfs, prec_vot_smfs, f1_vot_smfs = run_experiment(\n",
    "    dataset_name=\"JM1\",\n",
    "    model_name=\"Voting\",\n",
    "    smote_mode=\"SMOTUNED-DE\",\n",
    "    use_rfe=True,\n",
    "    n_features_to_select=12\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606dd714",
   "metadata": {},
   "source": [
    "Sammanst√§ll i tabell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768a3d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Sammanst√§ll alla resultat i en tabell\n",
    "\n",
    "results_all = [\n",
    "    # 1) LR & SVM ‚Äì no SMOTE, no RFE\n",
    "    (\"JM1\", \"Logistic Regression\", \"No\",        \"No\",        recall_lr_ns,  prec_lr_ns,  f1_lr_ns),\n",
    "    (\"JM1\", \"SVM\",                 \"No\",        \"No\",        recall_svm_ns, prec_svm_ns, f1_svm_ns),\n",
    "\n",
    "    # 2) LR & SVM ‚Äì basic SMOTE, no RFE\n",
    "    (\"JM1\", \"Logistic Regression\", \"SMOTE\",     \"No\",        recall_lr_sm,  prec_lr_sm,  f1_lr_sm),\n",
    "    (\"JM1\", \"SVM\",                 \"SMOTE\",     \"No\",        recall_svm_sm, prec_svm_sm, f1_svm_sm),\n",
    "\n",
    "    # 3) RF, XGB, Voting ‚Äì no SMOTE, no RFE\n",
    "    (\"JM1\", \"Random Forest\",       \"No\",        \"No\",        recall_rf_ns,  prec_rf_ns,  f1_rf_ns),\n",
    "    (\"JM1\", \"XGBoost\",             \"No\",        \"No\",        recall_xgb_ns, prec_xgb_ns, f1_xgb_ns),\n",
    "    (\"JM1\", \"Model Averaging\",     \"No\",        \"No\",        recall_vot_ns, prec_vot_ns, f1_vot_ns),\n",
    "\n",
    "    # 4) RF, XGB, Voting ‚Äì no SMOTE, with RFE\n",
    "    (\"JM1\", \"Random Forest\",       \"No\",        \"RFE\",       recall_rf_fs,  prec_rf_fs,  f1_rf_fs),\n",
    "    (\"JM1\", \"XGBoost\",             \"No\",        \"RFE\",       recall_xgb_fs, prec_xgb_fs, f1_xgb_fs),\n",
    "    (\"JM1\", \"Model Averaging\",     \"No\",        \"RFE\",       recall_vot_fs, prec_vot_fs, f1_vot_fs),\n",
    "\n",
    "    # 5) RF, XGB, Voting ‚Äì SMOTUNED-DE, with RFE\n",
    "    (\"JM1\", \"Random Forest\",       \"SMOTUNED\",  \"RFE\",       recall_rf_smfs,  prec_rf_smfs,  f1_rf_smfs),\n",
    "    (\"JM1\", \"XGBoost\",             \"SMOTUNED\",  \"RFE\",       recall_xgb_smfs, prec_xgb_smfs, f1_xgb_smfs),\n",
    "    (\"JM1\", \"Model Averaging\",     \"SMOTUNED\",  \"RFE\",       recall_vot_smfs, prec_vot_smfs, f1_vot_smfs),\n",
    "]\n",
    "\n",
    "df_results = pd.DataFrame(\n",
    "    results_all,\n",
    "    columns=[\"Dataset\", \"Model\", \"Oversampling\", \"Feature Selection\", \"Recall\", \"Precision\", \"F1\"]\n",
    ")\n",
    "\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225459fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost + SMOTUNED, utan RFE\n",
    "recall_xgb_no_fs, prec_xgb_no_fs, f1_xgb_no_fs = run_experiment(\n",
    "    dataset_name=\"JM1\",\n",
    "    model_name=\"XGBoost\",\n",
    "    smote_mode=\"SMOTUNED-DE\",\n",
    "    use_rfe=False\n",
    ")\n",
    "\n",
    "# XGBoost + SMOTUNED, med RFE\n",
    "recall_xgb_fs, prec_xgb_fs, f1_xgb_fs = run_experiment(\n",
    "    dataset_name=\"JM1\",\n",
    "    model_name=\"XGBoost\",\n",
    "    smote_mode=\"SMOTUNED-DE\",\n",
    "    use_rfe=True,\n",
    "    n_features_to_select=12\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aecf4c",
   "metadata": {},
   "source": [
    "Kod f√∂r ttest och 3-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5b3132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JM1 ‚Äì baseline vs ensemble\n",
    "f1_lr = cross_val_f1_scores(\n",
    "    dataset_name=\"JM1\",\n",
    "    model_name=\"LogisticRegression\",\n",
    "    smote_mode=\"NONE\",\n",
    "    use_rfe=False,\n",
    "    n_splits=3\n",
    ")\n",
    "\n",
    "f1_rf = cross_val_f1_scores(\n",
    "    dataset_name=\"JM1\",\n",
    "    model_name=\"RandomForest\",\n",
    "    smote_mode=\"SMOTUNED-DE\",\n",
    "    use_rfe=False,\n",
    "    n_splits=3\n",
    ")\n",
    "\n",
    "print(\"LR F1-scores:\", f1_lr)\n",
    "print(\"RF F1-scores:\", f1_rf)\n",
    "\n",
    "t_stat, p_val = ttest_rel(f1_lr, f1_rf)\n",
    "print(f\"Paired t-test: t = {t_stat:.4f}, p = {p_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88cf3da",
   "metadata": {},
   "source": [
    "Kod f√∂r cross-projekt. Dvs tr√§nar p√• KC1 och testar p√• KC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7257dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cpdp = cross_project_experiment(\n",
    "    train_dataset=\"KC1\",\n",
    "    test_dataset=\"KC2\",\n",
    "    model_name=\"RandomForest\",\n",
    "    smote_mode=\"SMOTUNED-DE\",\n",
    "    use_rfe=False  # b√∂rja utan RFE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9716fa17",
   "metadata": {},
   "source": [
    "Samma expriment men med och utan smotetuned och RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cd4909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CPDP: KC1 ‚Üí KC2 med RandomForest och olika SMOTE/RFE-l√§gen ===\n",
    "\n",
    "# 1) Baseline: ingen SMOTE, ingen RFE\n",
    "res_none = cross_project_experiment(\n",
    "    train_dataset=\"KC1\",\n",
    "    test_dataset=\"KC2\",\n",
    "    model_name=\"RandomForest\",\n",
    "    smote_mode=\"NONE\",\n",
    "    use_rfe=False\n",
    ")\n",
    "\n",
    "df_none = pd.DataFrame([{\n",
    "    \"Train ‚Üí Test\": \"KC1 ‚Üí KC2\",\n",
    "    \"Model\": \"RandomForest\",\n",
    "    \"Setting\": \"Baseline (NONE)\",\n",
    "    \"Recall\": res_none[\"recall\"],\n",
    "    \"Precision\": res_none[\"precision\"],\n",
    "    \"F1\": res_none[\"f1\"],\n",
    "    \"Accuracy\": res_none[\"accuracy\"],\n",
    "    \"AUC\": res_none[\"auc\"],\n",
    "}])\n",
    "\n",
    "print(\"\\n=== Tabell: KC1 ‚Üí KC2 ‚Äì Baseline (NONE) ===\")\n",
    "display(df_none)\n",
    "\n",
    "\n",
    "# 2) SMOTUNED-DE utan RFE\n",
    "res_smotuned = cross_project_experiment(\n",
    "    train_dataset=\"KC1\",\n",
    "    test_dataset=\"KC2\",\n",
    "    model_name=\"RandomForest\",\n",
    "    smote_mode=\"SMOTUNED-DE\",\n",
    "    use_rfe=False\n",
    ")\n",
    "\n",
    "df_smotuned = pd.DataFrame([{\n",
    "    \"Train ‚Üí Test\": \"KC1 ‚Üí KC2\",\n",
    "    \"Model\": \"RandomForest\",\n",
    "    \"Setting\": \"SMOTUNED-DE\",\n",
    "    \"Recall\": res_smotuned[\"recall\"],\n",
    "    \"Precision\": res_smotuned[\"precision\"],\n",
    "    \"F1\": res_smotuned[\"f1\"],\n",
    "    \"Accuracy\": res_smotuned[\"accuracy\"],\n",
    "    \"AUC\": res_smotuned[\"auc\"],\n",
    "}])\n",
    "\n",
    "print(\"\\n=== Tabell: KC1 ‚Üí KC2 ‚Äì SMOTUNED-DE (utan RFE) ===\")\n",
    "display(df_smotuned)\n",
    "\n",
    "\n",
    "# 3) SMOTUNED-DE + RFE (12 features)\n",
    "res_smotuned_rfe = cross_project_experiment(\n",
    "    train_dataset=\"KC1\",\n",
    "    test_dataset=\"KC2\",\n",
    "    model_name=\"RandomForest\",\n",
    "    smote_mode=\"SMOTUNED-DE\",\n",
    "    use_rfe=True,\n",
    "    n_features_to_select=12\n",
    ")\n",
    "\n",
    "df_smotuned_rfe = pd.DataFrame([{\n",
    "    \"Train ‚Üí Test\": \"KC1 ‚Üí KC2\",\n",
    "    \"Model\": \"RandomForest\",\n",
    "    \"Setting\": \"SMOTUNED-DE + RFE(12)\",\n",
    "    \"Recall\": res_smotuned_rfe[\"recall\"],\n",
    "    \"Precision\": res_smotuned_rfe[\"precision\"],\n",
    "    \"F1\": res_smotuned_rfe[\"f1\"],\n",
    "    \"Accuracy\": res_smotuned_rfe[\"accuracy\"],\n",
    "    \"AUC\": res_smotuned_rfe[\"auc\"],\n",
    "}])\n",
    "\n",
    "print(\"\\n=== Tabell: KC1 ‚Üí KC2 ‚Äì SMOTUNED-DE + RFE(12) ===\")\n",
    "display(df_smotuned_rfe)\n",
    "\n",
    "\n",
    "# (Extra) Kombinera alla tre i EN tabell om du vill klistra in i rapporten\n",
    "df_kc1_kc2_all = pd.concat([df_none, df_smotuned, df_smotuned_rfe], ignore_index=True)\n",
    "df_kc1_kc2_all[[\"Recall\", \"Precision\", \"F1\", \"Accuracy\", \"AUC\"]] = df_kc1_kc2_all[[\"Recall\", \"Precision\", \"F1\", \"Accuracy\", \"AUC\"]].round(4)\n",
    "\n",
    "print(\"\\n=== Samlad tabell ‚Äì KC1 ‚Üí KC2, RandomForest, tre inst√§llningar ===\")\n",
    "display(df_kc1_kc2_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000b6971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline CPDP utan SMOTE\n",
    "res_rf_cpdp_no = cross_project_experiment(\n",
    "    train_dataset=\"KC1\",\n",
    "    test_dataset=\"KC2\",\n",
    "    model_name=\"RandomForest\",\n",
    "    smote_mode=\"NONE\",\n",
    "    use_rfe=False\n",
    ")\n",
    "\n",
    "# Din nuvarande (SMOTUNED)\n",
    "res_rf_cpdp_sm = cross_project_experiment(\n",
    "    train_dataset=\"KC1\",\n",
    "    test_dataset=\"KC2\",\n",
    "    model_name=\"RandomForest\",\n",
    "    smote_mode=\"SMOTUNED-DE\",\n",
    "    use_rfe=False  # eller True om du vill RFE h√§r ocks√•\n",
    ")\n",
    "\n",
    "res_rf_cpdp_sm_rfe = cross_project_experiment(\n",
    "    train_dataset=\"KC1\",\n",
    "    test_dataset=\"KC2\",\n",
    "    model_name=\"RandomForest\",\n",
    "    smote_mode=\"SMOTUNED-DE\",\n",
    "    use_rfe=True,\n",
    "    n_features_to_select=12\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e7bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: JM1 + RandomForest utan RFE\n",
    "df_no_rfe, pivot_no_rfe = compare_smote_variants(\n",
    "    \"JM1\",\n",
    "    \"RandomForest\",\n",
    "    use_rfe=False  # ingen feature selection\n",
    ")\n",
    "\n",
    "df_no_rfe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fe6316",
   "metadata": {},
   "source": [
    "Kod f√∂r smote varianter MED RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6112189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: JM1 + RandomForest med RFE (t.ex. 12 features)\n",
    "df_rfe, pivot_rfe = compare_smote_variants(\n",
    "    \"KC1\",\n",
    "    \"RandomForest\",\n",
    "    use_rfe=True,          # aktivera RFE\n",
    "    n_features_to_select=12  # antal features du beh√•ller\n",
    ")\n",
    "\n",
    "df_rfe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ad9293",
   "metadata": {},
   "source": [
    "Kod f√∂r tabell j√§mf√∂relse av dataset + modell med/utan RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97fe658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# J√§mf√∂r JM1 + RandomForest med och utan RFE\n",
    "\n",
    "df_no_rfe_tagged = df_no_rfe.copy().set_index(\"smote_mode\")\n",
    "df_rfe_tagged = df_rfe.copy().set_index(\"smote_mode\")\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"F1_no_rfe\": df_no_rfe_tagged[\"f1\"],\n",
    "    \"F1_rfe\": df_rfe_tagged[\"f1\"],\n",
    "    \"Recall_no_rfe\": df_no_rfe_tagged[\"recall\"],\n",
    "    \"Recall_rfe\": df_rfe_tagged[\"recall\"],\n",
    "})\n",
    "\n",
    "comparison[\"delta_F1\"] = comparison[\"F1_rfe\"] - comparison[\"F1_no_rfe\"]\n",
    "comparison[\"delta_recall\"] = comparison[\"Recall_rfe\"] - comparison[\"Recall_no_rfe\"]\n",
    "\n",
    "comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc4c27b",
   "metadata": {},
   "source": [
    "F√∂r att k√∂ra via menyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448e9d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_df = run_experiments_menu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d86bed",
   "metadata": {},
   "source": [
    "S√• h√§r tolkar du den:\n",
    "Varje rad = samma modell + samma dataset, men olika SMOTE-uppl√§gg\n",
    "NONE: ingen SMOTE (original obalanserad tr√§ning).\n",
    "BASIC: standard-SMOTE.\n",
    "GRID: SMOTE med grid-tunade parametrar.\n",
    "SMOTUNED-DE: SMOTE med DE-optimerade parametrar.\n",
    "\n",
    "Kolumnerna:\n",
    "accuracy ‚Üí andel r√§tt totalt.\n",
    "‚Üí kan vara missvisande p√• obalanserad data, f√∂r att en modell kan f√• h√∂g accuracy genom att n√§stan alltid gissa 0.\n",
    "precision (f√∂r klass 1) ‚Üí ‚Äún√§r modellen s√§ger defekt, hur ofta har den r√§tt?‚Äù\n",
    "recall (f√∂r klass 1) ‚Üí ‚Äúhur stor andel av alla verkliga defekter hittar modellen?‚Äù\n",
    "f1 ‚Üí balans mellan precision & recall (bra huvudm√•tt).\n",
    "auc ‚Üí hur bra modellen rangordnar defekter vs icke-defekter (threshold-oberoende)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
