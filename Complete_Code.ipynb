{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ba564e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Del 1: Importera n√∂dv√§ndiga bibliotek ===\n",
    "\n",
    "import os  # Hantera filer, mappar och s√∂kv√§gar\n",
    "\n",
    "import pandas as pd  # Pandas: l√§sa och hantera tabell-data (CSV osv)\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # Dela upp data i train/test\n",
    "from sklearn.preprocessing import StandardScaler      # Skala numeriska features\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression   # Logistisk regression (klassificering)\n",
    "from sklearn.svm import SVC                           # Support Vector Classifier (SVM-modell)\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "# RandomForestClassifier: ensemble av m√•nga decision trees\n",
    "# VotingClassifier: kombinerar flera modeller och l√•ter dem \"r√∂sta\"\n",
    "\n",
    "from xgboost import XGBClassifier                     # XGBoost-modell (gradient boosting)\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    ")\n",
    "# Dessa funktioner r√§knar ut utv√§rderingsm√•tt:\n",
    "# accuracy, precision, recall, F1, AUC, samt en sammanfattande rapport\n",
    "\n",
    "from imblearn.over_sampling import SMOTE              # SMOTE: √∂versampling f√∂r obalanserade klasser\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc362a99",
   "metadata": {},
   "source": [
    "import os : ger funktion f√∂r att kunna jobba med filer\n",
    "import pandas: standardbiblotek f√∂r att l√§sa/skriva och hantera tabell-data\n",
    "train_test_split: delar upp data i tr√§ning/test med stratifiering dvs den beh√•ller samma ratio exepelvis 80/20 i b√•de test och tr√§ningsdata.\n",
    "Standardscalar. Skalar kolumnerna (features) s√• att de f√•r medelv√§rdet 0 och standardavikelse 1.\n",
    "Logistig Regression: Enkel linj√§r klassifieringsmodell\n",
    "SVc: Support Vector Machine , enkel klassifierare (extra)\n",
    "RandomForestClassifier: Ensemblemodell av m√•nga beslutstr√§d. Vanlig Baseline\n",
    "VotingClassifier: en kombination av olika modeller ex: LogReg +RF+XGB och ta majoritetsr√∂st (extra)\n",
    "XGBClassifier: gradient boosting modell\n",
    "\n",
    "Accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report : funktioner f√∂r att r√§kna utv√§rderingsm√•tt\n",
    "\n",
    "SMOTE: syntetiskt skapar fler minoritets-exempel i tr√§ningsdataset(endast p√• train delen INTE test) f√∂r att hantera obalanserade klasser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc4102f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arbetskatalog: c:\\Users\\josef\\OneDrive\\Desktop\\Thesis\\notebooks\n",
      "Data-mapp: ..\\data\n",
      "S√∂kv√§g till filen: ..\\data\\cm1.csv\n",
      "Shape (rader, kolumner): (498, 22)\n",
      "\n",
      "Kolumner:\n",
      "Index(['loc', 'v(g)', 'ev(g)', 'iv(g)', 'n', 'v', 'l', 'd', 'i', 'e', 'b', 't',\n",
      "       'lOCode', 'lOComment', 'lOBlank', 'locCodeAndComment', 'uniq_Op',\n",
      "       'uniq_Opnd', 'total_Op', 'total_Opnd', 'branchCount', 'defects'],\n",
      "      dtype='object')\n",
      "\n",
      "Klassf√∂rdelning i 'defects':\n",
      "defects\n",
      "False    449\n",
      "True      49\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# === Del 2: Grundinst√§llningar ===\n",
    "\n",
    "RANDOM_STATE = 42  # F√∂r reproducerbara resultat (samma slump varje g√•ng)\n",
    "\n",
    "# Om din notebook ligger i mappen \"Thesis/notebooks\":\n",
    "DATA_DIR = os.path.join(\"..\", \"data\")\n",
    "\n",
    "# Om din notebook ligger direkt i \"Thesis\"-mappen ist√§llet, anv√§nd i st√§llet:\n",
    "# DATA_DIR = \"data\"\n",
    "\n",
    "print(\"Arbetskatalog:\", os.getcwd())\n",
    "print(\"Data-mapp:\", DATA_DIR)\n",
    "\n",
    "# === Del 3: Ladda in ett NASA-dataset (CM1) ===\n",
    "\n",
    "cm1_path = os.path.join(DATA_DIR, \"cm1.csv\")\n",
    "print(\"S√∂kv√§g till filen:\", cm1_path)\n",
    "\n",
    "# L√§sa in csv-filen med pandas\n",
    "cm1 = pd.read_csv(cm1_path)\n",
    "\n",
    "# Snabb √∂verblick\n",
    "print(\"Shape (rader, kolumner):\", cm1.shape)\n",
    "print(\"\\nKolumner:\")\n",
    "print(cm1.columns)\n",
    "\n",
    "# Kolla hur m√•lkategorin ser ut (just nu gissar vi att den heter 'defects')\n",
    "if \"defects\" in cm1.columns:\n",
    "    print(\"\\nKlassf√∂rdelning i 'defects':\")\n",
    "    print(cm1[\"defects\"].value_counts())\n",
    "else:\n",
    "    print(\"\\nHittade ingen kolumn med namnet 'defects' ‚Äì vi f√•r kolla vad target heter.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a9ce80",
   "metadata": {},
   "source": [
    "os.path.join(DATA_DIR, \"cm1.csv\") skapar r√§tt filv√§g\n",
    "pb.read_csv(...) l√§ser in filen till en DataFrame vi skriver ut: Shape antal rader/kolumner, columnnamn och evetuell klassf√∂rdelning om target heter defect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce8d7bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (498, 21)\n",
      "y shape: (498,)\n",
      "\n",
      "Klassf√∂rdelning i y:\n",
      "defects\n",
      "False    449\n",
      "True      49\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# === Del 4: Dela upp data i X (features) och y (m√•lvariabel) ===\n",
    "\n",
    "TARGET_COL = \"defects\"  # √§ndra h√§r om din kolumn heter n√•got annat\n",
    "\n",
    "# X = alla kolumner utom target\n",
    "X = cm1.drop(columns=[TARGET_COL])\n",
    "\n",
    "# y = bara target-kolumnen\n",
    "y = cm1[TARGET_COL]\n",
    "\n",
    "print(\"X shape:\", X.shape)  # (antal rader, antal features)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"\\nKlassf√∂rdelning i y:\")\n",
    "print(y.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d052031",
   "metadata": {},
   "source": [
    "drop(columns= [TARGET_COL]) vi plockar bort defects och sparar resten som indata.\n",
    "y=cm1[TARGET_COL] vector med 0/1 dvs defekt/icke-defekt\n",
    "value_counts () visar hur obalanserad datan √§r, viktig att notera f√∂re smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7eee2c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (398, 21) (398,)\n",
      "Test shape: (100, 21) (100,)\n",
      "\n",
      "Train klassf√∂rdelning:\n",
      "defects\n",
      "False    0.90201\n",
      "True     0.09799\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test klassf√∂rdelning:\n",
      "defects\n",
      "False    0.9\n",
      "True     0.1\n",
      "Name: proportion, dtype: float64\n",
      "X_train_scaled shape: (398, 21)\n",
      "X_test_scaled shape: (100, 21)\n"
     ]
    }
   ],
   "source": [
    "# === Del 5: Train-test-split med stratifiering ===\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # redan importerat, men skadar inte\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,       # 80% tr√§ning, 20% test\n",
    "    random_state=42,     # samma slump varje g√•ng\n",
    "    stratify=y           # bevara klassf√∂rdelning i b√•de train och test\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test shape:\", X_test.shape, y_test.shape)\n",
    "print(\"\\nTrain klassf√∂rdelning:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nTest klassf√∂rdelning:\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# === Del 6: Skala features med StandardScaler ===\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit p√• tr√§ningsdatan, transform b√•de train och test\n",
    "# fit_tranform l√§r sig parametrar (medelv√§rde, std) fr√•n X_train och\n",
    "# anv√§nder dessa parametrar f√∂r att skala datan\n",
    "# detta g√∂r \n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)  #\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"X_test_scaled shape:\", X_test_scaled.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0abaf47",
   "metadata": {},
   "source": [
    "N√§r vi normaliserar (t.ex. med StandardScaler) g√∂r vi s√• att alla features f√•r ungef√§r:\n",
    "\n",
    "medelv√§rde ‚âà 0\n",
    "spridning (standardavvikelse) ‚âà 1\n",
    "\n",
    "Det betyder att:\n",
    "alla kolumner hamnar p√• liknande skala\n",
    "ingen feature ‚Äúdominerar‚Äù bara f√∂r att den r√•kar ha stora tal\n",
    "Modeller som Logistisk regression, SVM och neurala n√§t fungerar d√•:\n",
    "stabilare\n",
    "snabbare\n",
    "och ger oftast b√§ttre resultat.\n",
    "\n",
    "\n",
    "Vi skalar om alla features s√• att de har liknande storlek ist√§llet f√∂r att vissa √§r s√§ttestora och andra j√§tteasm√•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b49822",
   "metadata": {},
   "source": [
    "test_size=0.2 betyder att 20% av datan sparas som test dvs den r√∂rs inte av SMOTE eller tr√§ning. \n",
    "stratify=y samma propwertion 0/1 i b√•de train data och test data\n",
    "vi skriver ut storleken och dess klassf√∂rdelning f√∂r att verifiers.\n",
    "\n",
    "eli5: 21 st kolumner √§r input och 1 √§r output = defects\n",
    "N√§r vi tr√§nar en modell beh√∂ver den veta vad √§r input -> X och vad det r√§tta svaret √§r f√∂r varje rad -> y\n",
    "d√§rf√∂r √§r x alla kolumner som modellen anv√§nder f√∂r att g√∂ra en prediction, kolumnen defects √§r facit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5d52a0",
   "metadata": {},
   "source": [
    "Genom klassf√∂rdelning kontrollerar vi 3 grejer\n",
    "\n",
    "A vi kontrollerar att vi har r√§tt kolumn som target det ser vi genom att vi ser tv√• v√§rden och verkar vara bin√§rt?\n",
    "\n",
    "B Vi ser hur obalanserad datan √§r genom value_counts() som s√§ger hur m√•nga fler 0 √§n 1 vi har exempelvis 90% klass 0 och 10% klass 1 \n",
    "Detta hj√§lper varf√∂r vi beh√∂ver smote och hj√§lper oss att f√∂rst√• resultatet\n",
    "\n",
    "C Vi kan j√§mf√∂ra f√∂re och efter split och SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccc614f",
   "metadata": {},
   "source": [
    "F√∂re split & SMOTE (originaldata)\n",
    "Genom y.value_counts() ser vi snabbt hur m√•nga som √§r majoritetsklassen (0) och minoriteten (1).\n",
    "Det visar hur obalanserad datan √§r innan vi g√∂r n√•got alls.\n",
    "\n",
    "\n",
    "Efter train/test-split\n",
    "Vi skriver ut y_train.value_counts() och y_test.value_counts() f√∂r att\n",
    "se att f√∂rdelningen 0/1 √§r liknande i b√•de train och test.\n",
    "Det bekr√§ftar att stratify=y fungerade.\n",
    "\n",
    "\n",
    "Efter SMOTE (p√• tr√§ningen)\n",
    "Vi k√∂r SMOTE p√• X_train, y_train och skriver sedan ut y_train_smote.value_counts().\n",
    "\n",
    "Nu kan vi se att majoritetsklassen och minoritetsklassen √§r lika stora ‚Üí datan √§r balanserad f√∂r tr√§ningen.\n",
    "Sedan j√§mf√∂r vi modellerna\n",
    "\n",
    "Vi kan j√§mf√∂ra modellernas resultat:\n",
    "\n",
    "p√• obalanserad data (f√∂re SMOTE)\n",
    "\n",
    "mot balanserad tr√§ning (efter SMOTE)\n",
    "\n",
    "D√• ser vi hur mycket SMOTE faktiskt f√∂rb√§ttrar t.ex. recall och F1 f√∂r minoritetsklassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7217e8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F√∂re SMOTE: (398, 21) klassf√∂rdelning:\n",
      "defects\n",
      "False    359\n",
      "True      39\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Efter SMOTE: (718, 21) klassf√∂rdelning:\n",
      "defects\n",
      "False    359\n",
      "True     359\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# === Del 7: Hantera obalans med SMOTE (endast p√• train) ===\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(\"F√∂re SMOTE:\", X_train_scaled.shape, \"klassf√∂rdelning:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"\\nEfter SMOTE:\", X_train_smote.shape, \"klassf√∂rdelning:\")\n",
    "print(pd.Series(y_train_smote).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04564ae",
   "metadata": {},
   "source": [
    "Fit_resample skapar syntetiska minoritets exempel tills  b√•da klasserna har ungef√§r samma antal. Vi j√§mf√∂r sedan nshape och value counts f√∂r att observeras sdkillnaden f√∂re/efter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e89e9f",
   "metadata": {},
   "source": [
    "Vi skriver ut en funktion som hj√§lper oss att utv√§dera modellerna baserat p√• vad den har tr√§nar p√•, vad den gissar p√• och vad som √§r det r√§tta svaret.\n",
    "\n",
    "steg f√∂r steg\n",
    "1. delar datan i test och train i samma ratio\n",
    "2. Vi skalar och anv√§nder SMOTE p√• tr√§ningsdatan\n",
    "3. Vi tr√§nar modellen p√• tr√§ningsdatan \n",
    "4. Vi utv√§rderar modellen mot testdatan dvs y_test som √§r facit. Modellen gissar p√• testdatan med y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16876264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Del 8: Hj√§lpfunktion f√∂r att utv√§rdera modeller ===\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Tr√§nad modell + testdata -> returnerar en dict med metrics.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # ber√§kna sannolikheter f√∂r klass 1 om m√∂jligt\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        # vissa modeller har decision_function ist√§llet\n",
    "        y_proba = model.decision_function(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"y_proba\": y_proba,\n",
    "    }\n",
    "# === Del 9: Tr√§na och utv√§rdera olika modeller ===\n",
    "# Logistisk Regression\n",
    "log_reg = LogisticRegression(\n",
    "    max_iter=1000,     # till√•ter fler iterationer s√• den hinner konvergera\n",
    "    n_jobs=-1          # anv√§nd alla k√§rnor\n",
    ")\n",
    "\n",
    "log_reg.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "log_results = evaluate_model(log_reg, X_test_scaled, y_test)\n",
    "\n",
    "print(\"Logistic Regression ‚Äì CM1 (med SMOTE)\")\n",
    "for k, v in log_results.items():\n",
    "    if k in [\"y_pred\", \"y_proba\"]:\n",
    "        continue\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# === Del 10: Tr√§na Random Forest p√• CM1 (med SMOTE) ===\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "rf_results = evaluate_model(rf, X_test_scaled, y_test)\n",
    "\n",
    "print(\"Random Forest ‚Äì CM1 (med SMOTE)\")\n",
    "for k, v in rf_results.items():\n",
    "    if k in [\"y_pred\", \"y_proba\"]:\n",
    "        continue\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca1d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "F√∂rklaring:\n",
    "Vi tr√§nar modellen p√• balanserad tr√§ning (X_train_smote, y_train_smote).\n",
    "Vi utv√§rderar p√• originell, obalanserad testdata (X_test_scaled, y_test).\n",
    "Sedan skriver vi ut accuracy, precision, recall, f1, auc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac8a684",
   "metadata": {},
   "source": [
    "KOMPLETT KOD INKLUSIVE MENYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a5cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Del 1: Imports & grundinst√§llningar ===\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.base import clone\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from scipy.optimize import differential_evolution\n",
    "from IPython.display import display\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Anpassa beroende p√• var din notebook ligger:\n",
    "# - om notebook ligger i Thesis/notebooks ‚Üí \".. / data\"\n",
    "# - om notebook ligger direkt i Thesis ‚Üí \"data\"\n",
    "DATA_DIR = os.path.join(\"..\", \"data\")\n",
    "# DATA_DIR = \"data\"  # anv√§nd denna ist√§llet om notebooken ligger i rotmappen\n",
    "\n",
    "\n",
    "# === Del 2: utv√§rderingsfunktion ===\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Tar en TR√ÑNAD modell + testdata och r√§knar nyckeltal.\n",
    "    Returnerar en dict med metrics + y_pred/y_proba.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_proba = model.decision_function(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"y_proba\": y_proba,\n",
    "    }\n",
    "\n",
    "\n",
    "# === Del 3: Dataset-konfiguration & f√∂rberedelse ===\n",
    "\n",
    "DATASETS = {\n",
    "    \"JM1\": {\"filename\": \"jm1.csv\", \"target\": \"defects\"},\n",
    "    \"KC1\": {\"filename\": \"kc1.csv\", \"target\": \"defects\"},\n",
    "    \"KC2\": {\"filename\": \"kc2.csv\", \"target\": \"defects\"},\n",
    "    \"PC1\": {\"filename\": \"pc1.csv\", \"target\": \"defects\"},\n",
    "    \"CM1\": {\"filename\": \"cm1.csv\", \"target\": \"defects\"},\n",
    "}\n",
    "\n",
    "def load_and_prepare_dataset(dataset_name):\n",
    "    \"\"\"\n",
    "    L√§ser in valt dataset, delar i train/test, skalar features.\n",
    "    SMOTE g√∂r vi separat beroende p√• menyval.\n",
    "    \"\"\"\n",
    "    info = DATASETS[dataset_name]\n",
    "    path = os.path.join(DATA_DIR, info[\"filename\"])\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    target_col = info[\"target\"]\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    # s√§kerst√§ll 0/1\n",
    "    if y.dtype == \"bool\":\n",
    "        y = y.astype(int)\n",
    "    elif y.dtype == \"object\":\n",
    "        y = y.str.lower().map({\"yes\": 1, \"true\": 1, \"defective\": 1}).fillna(0).astype(int)\n",
    "\n",
    "    print(f\"{dataset_name}: shape={df.shape}\")\n",
    "    print(\"Klassf√∂rdelning (hela datan):\")\n",
    "    print(y.value_counts(), \"\\n\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        stratify=y,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    print(\"Train klassf√∂rdelning:\")\n",
    "    print(y_train.value_counts())\n",
    "    print(\"\\nTest klassf√∂rdelning:\")\n",
    "    print(y_test.value_counts(), \"\\n\")\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "\n",
    "# === Del 4: Modeller (alla basmodeller) ===\n",
    "\n",
    "def get_base_models():\n",
    "    \"\"\"\n",
    "    Skapar alla modeller vi vill testa.\n",
    "    \"\"\"\n",
    "    log_reg = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    ann = MLPClassifier(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation=\"relu\",\n",
    "        max_iter=200,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    svc = SVC(\n",
    "        kernel=\"rbf\",\n",
    "        probability=True,   # beh√∂vs f√∂r AUC\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    voting = VotingClassifier(\n",
    "        estimators=[\n",
    "            (\"logreg\", log_reg),\n",
    "            (\"rf\", rf),\n",
    "            (\"xgb\", xgb),\n",
    "        ],\n",
    "        voting=\"soft\"  # anv√§nder sannolikheter\n",
    "    )\n",
    "\n",
    "    models = {\n",
    "        \"LogisticRegression\": log_reg,\n",
    "        \"RandomForest\": rf,\n",
    "        \"XGBoost\": xgb,\n",
    "        \"ANN\": ann,\n",
    "        \"SVC\": svc,\n",
    "        \"Voting\": voting,\n",
    "    }\n",
    "    return models\n",
    "\n",
    "\n",
    "# === Del 5: SMOTE-varianter ===\n",
    "\n",
    "def apply_basic_smote(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Standard-SMOTE med default-parametrar.\n",
    "    \"\"\"\n",
    "    smote = SMOTE(random_state=RANDOM_STATE)\n",
    "    X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "    print(\"Efter basic SMOTE:\")\n",
    "    print(pd.Series(y_res).value_counts(), \"\\n\")\n",
    "    return X_res, y_res\n",
    "\n",
    "\n",
    "def smote_grid_search(model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Enkel grid search p√• SMOTE-parametrar (inspirerad av SMOTUNED-id√©n).\n",
    "    \"\"\"\n",
    "    pipe = Pipeline([\n",
    "        (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
    "        (\"clf\", model),\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        \"smote.k_neighbors\": [3, 5, 7],\n",
    "        \"smote.sampling_strategy\": [0.5, 0.75, 1.0],\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        pipe,\n",
    "        param_grid,\n",
    "        scoring=\"f1\",\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(\"GRID-SMOTE ‚Äì b√§sta parametrar:\", grid.best_params_)\n",
    "    return grid.best_estimator_\n",
    "\n",
    "\n",
    "def smotuned_de(model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    F√∂renklad SMOTUNED-id√©:\n",
    "    differential evolution optimerar SMOTE-parametrar (k_neighbors, sampling_strategy)\n",
    "    f√∂r att maximera F1 med 3-fold CV.\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(params):\n",
    "        # params = [k_neighbors, sampling_strategy]\n",
    "        k = int(round(params[0]))\n",
    "        k = max(2, min(k, 15))   # h√•ll k inom [2, 15]\n",
    "\n",
    "        sampling = float(params[1])\n",
    "        sampling = max(0.2, min(sampling, 1.0))  # sampling_strategy inom [0.2, 1.0]\n",
    "\n",
    "        smote = SMOTE(\n",
    "            k_neighbors=k,\n",
    "            sampling_strategy=sampling,\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "\n",
    "        X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scores = []\n",
    "\n",
    "        for train_idx, val_idx in cv.split(X_res, y_res):\n",
    "            X_tr, X_val = X_res[train_idx], X_res[val_idx]\n",
    "            y_tr, y_val = y_res[train_idx], y_res[val_idx]\n",
    "\n",
    "            m = clone(model)\n",
    "            m.fit(X_tr, y_tr)\n",
    "            y_pred = m.predict(X_val)\n",
    "            scores.append(f1_score(y_val, y_pred, zero_division=0))\n",
    "\n",
    "        # differential_evolution minimerar, s√• vi returnerar -F1\n",
    "        return -np.mean(scores)\n",
    "\n",
    "    bounds = [\n",
    "        (2, 15),    # k_neighbors\n",
    "        (0.2, 1.0), # sampling_strategy\n",
    "    ]\n",
    "\n",
    "    result = differential_evolution(\n",
    "        objective,\n",
    "        bounds,\n",
    "        maxiter=15,\n",
    "        popsize=10,\n",
    "        tol=0.01,\n",
    "        polish=True,\n",
    "        disp=False,\n",
    "    )\n",
    "\n",
    "    best_k = int(round(result.x[0]))\n",
    "    best_sampling = float(result.x[1])\n",
    "    best_k = max(2, min(best_k, 15))\n",
    "    best_sampling = max(0.2, min(best_sampling, 1.0))\n",
    "\n",
    "    print(\"SMOTUNED-DE ‚Äì b√§sta parametrar:\")\n",
    "    print(\"k_neighbors:\", best_k)\n",
    "    print(\"sampling_strategy:\", best_sampling)\n",
    "\n",
    "    best_smote = SMOTE(\n",
    "        k_neighbors=best_k,\n",
    "        sampling_strategy=best_sampling,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "    X_res_best, y_res_best = best_smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    final_model = clone(model)\n",
    "    final_model.fit(X_res_best, y_res_best)\n",
    "\n",
    "    return final_model\n",
    "\n",
    "\n",
    "# === Del 6: Meny f√∂r att k√∂ra experiment ===\n",
    "\n",
    "def run_experiments_menu():\n",
    "    # v√§lj dataset\n",
    "    print(\"Tillg√§ngliga dataset:\")\n",
    "    dataset_names = list(DATASETS.keys())\n",
    "    for idx, name in enumerate(dataset_names, start=1):\n",
    "        print(f\"{idx} = {name}\")\n",
    "    print(\"ALL = alla dataset\")\n",
    "\n",
    "    dataset_choice = input(\"V√§lj dataset (t.ex. 1, 2, 3 eller JM1/KC1/ALL): \").strip().upper()\n",
    "\n",
    "    all_datasets_selected = False\n",
    "\n",
    "    if dataset_choice == \"ALL\":\n",
    "        datasets_to_run = dataset_names\n",
    "        all_datasets_selected = True\n",
    "    elif dataset_choice.isdigit():\n",
    "        idx = int(dataset_choice) - 1\n",
    "        if 0 <= idx < len(dataset_names):\n",
    "            datasets_to_run = [dataset_names[idx]]\n",
    "        else:\n",
    "            print(\"Ogiltigt sifferval, anv√§nder f√∂rsta datasetet.\")\n",
    "            datasets_to_run = [dataset_names[0]]\n",
    "    else:\n",
    "        # anta att anv√§ndaren skrev namnet direkt, t.ex. JM1\n",
    "        if dataset_choice in DATASETS:\n",
    "            datasets_to_run = [dataset_choice]\n",
    "        else:\n",
    "            print(\"Ogiltigt namn, anv√§nder f√∂rsta datasetet.\")\n",
    "            datasets_to_run = [dataset_names[0]]\n",
    "\n",
    "    # v√§lj modell(er)\n",
    "    models = get_base_models()\n",
    "    print(\"\\nTillg√§ngliga modeller:\")\n",
    "    model_names = list(models.keys())\n",
    "    for idx, name in enumerate(model_names, start=1):\n",
    "        print(f\"{idx} = {name}\")\n",
    "    print(\"ALL = alla modeller\")\n",
    "\n",
    "    model_choice = input(\"V√§lj modell (t.ex. 1, 2 eller RandomForest/ALL): \").strip()\n",
    "\n",
    "    all_models_selected = False\n",
    "\n",
    "    if model_choice.upper() == \"ALL\":\n",
    "        model_names_to_run = model_names\n",
    "        all_models_selected = True\n",
    "    elif model_choice.isdigit():\n",
    "        idx = int(model_choice) - 1\n",
    "        if 0 <= idx < len(model_names):\n",
    "            model_names_to_run = [model_names[idx]]\n",
    "        else:\n",
    "            print(\"Ogiltigt sifferval, anv√§nder f√∂rsta modellen.\")\n",
    "            model_names_to_run = [model_names[0]]\n",
    "    else:\n",
    "        if model_choice in models:\n",
    "            model_names_to_run = [model_choice]\n",
    "        else:\n",
    "            print(\"Ogiltigt modellnamn, anv√§nder f√∂rsta modellen.\")\n",
    "            model_names_to_run = [model_names[0]]\n",
    "\n",
    "    # v√§lj SMOTE-l√§ge\n",
    "    print(\"\\nSMOTE-l√§gen:\")\n",
    "    print(\"0 = Ingen SMOTE\")\n",
    "    print(\"1 = Basic SMOTE (standardparametrar)\")\n",
    "    print(\"2 = GRID-SMOTE (enkel tuning)\")\n",
    "    print(\"3 = SMOTUNED-DE (evolution√§r tuning)\")\n",
    "    print(\"4 = J√§mf√∂r ALLA SMOTE-varianter f√∂r vald dataset + modell\")\n",
    "    smote_mode = input(\"V√§lj 0 / 1 / 2 / 3 / 4: \").strip()\n",
    "\n",
    "    # üî∏ Specialfall: smote_mode 4 = k√∂r compare_smote_variants f√∂r EN kombination\n",
    "    if smote_mode == \"4\":\n",
    "        if len(datasets_to_run) == 1 and len(model_names_to_run) == 1:\n",
    "            ds = datasets_to_run[0]\n",
    "            mn = model_names_to_run[0]\n",
    "            df_compare, pivot_compare = compare_smote_variants(ds, mn)\n",
    "            return df_compare\n",
    "        else:\n",
    "            print(\"\\n‚ö† SMOTE-l√§ge 4 kr√§ver att du v√§ljer EXAKT ett dataset och en modell (inte ALL).\")\n",
    "            print(\"Byter till l√§ge 1 (Basic SMOTE) ist√§llet.\\n\")\n",
    "            smote_mode = \"1\"\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for ds in datasets_to_run:\n",
    "        print(\"\\n==============================\")\n",
    "        print(f\"K√∂r dataset: {ds}\")\n",
    "        print(\"==============================\\n\")\n",
    "\n",
    "        X_train_scaled, X_test_scaled, y_train, y_test = load_and_prepare_dataset(ds)\n",
    "\n",
    "        for model_name in model_names_to_run:\n",
    "            base_models = get_base_models()  # nya instanser\n",
    "            model = base_models[model_name]\n",
    "\n",
    "            print(f\"\\n--- Modell: {model_name} ---\")\n",
    "\n",
    "            # v√§lj tr√§ningsstrategi beroende p√• smote_mode\n",
    "            if smote_mode == \"0\":\n",
    "                print(\"Ingen SMOTE anv√§nds.\\n\")\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                used_model = model\n",
    "                smote_label = \"NONE\"\n",
    "\n",
    "            elif smote_mode == \"1\":\n",
    "                X_train_smote, y_train_smote = apply_basic_smote(X_train_scaled, y_train)\n",
    "                model.fit(X_train_smote, y_train_smote)\n",
    "                used_model = model\n",
    "                smote_label = \"BASIC\"\n",
    "\n",
    "            elif smote_mode == \"2\":\n",
    "                used_model = smote_grid_search(model, X_train_scaled, y_train)\n",
    "                smote_label = \"GRID\"\n",
    "\n",
    "            elif smote_mode == \"3\":\n",
    "                used_model = smotuned_de(model, X_train_scaled, y_train)\n",
    "                smote_label = \"SMOTUNED-DE\"\n",
    "\n",
    "            else:\n",
    "                print(\"Ogiltigt SMOTE-val, anv√§nder ingen SMOTE.\")\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                used_model = model\n",
    "                smote_label = \"NONE\"\n",
    "\n",
    "            # utv√§rdera\n",
    "            eval_results = evaluate_model(used_model, X_test_scaled, y_test)\n",
    "            print(f\"Resultat ‚Äì {ds} ‚Äì {model_name} ‚Äì SMOTE-l√§ge {smote_label}\")\n",
    "            for k, v in eval_results.items():\n",
    "                if k in [\"y_pred\", \"y_proba\"]:\n",
    "                    continue\n",
    "                print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "            all_results.append({\n",
    "                \"dataset\": ds,\n",
    "                \"model\": model_name,\n",
    "                \"smote_mode\": smote_label,\n",
    "                \"accuracy\": eval_results[\"accuracy\"],\n",
    "                \"precision\": eval_results[\"precision\"],\n",
    "                \"recall\": eval_results[\"recall\"],\n",
    "                \"f1\": eval_results[\"f1\"],\n",
    "                \"auc\": eval_results[\"auc\"],\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(\"\\n=== Sammanfattning av alla k√∂rningar ===\")\n",
    "    display(results_df)\n",
    "\n",
    "    # fortfarande: om du k√∂r ALL + ALL kan pivot-tabell vara nice\n",
    "    if all_datasets_selected and all_models_selected and not results_df.empty:\n",
    "        pivot_f1 = results_df.pivot_table(\n",
    "            index=[\"dataset\", \"model\"],\n",
    "            columns=\"smote_mode\",\n",
    "            values=\"f1\"\n",
    "        )\n",
    "        print(\"\\n=== F1 per dataset/modell och SMOTE-l√§ge ===\")\n",
    "        display(pivot_f1)\n",
    "\n",
    "    return results_df\n",
    "results_df = run_experiments_menu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cae001",
   "metadata": {},
   "source": [
    "KOMPLETT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "097df61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tillg√§ngliga dataset:\n",
      "1 = JM1\n",
      "2 = KC1\n",
      "3 = KC2\n",
      "4 = PC1\n",
      "5 = CM1\n",
      "ALL = alla dataset\n",
      "\n",
      "Tillg√§ngliga modeller:\n",
      "1 = LogisticRegression\n",
      "2 = RandomForest\n",
      "3 = XGBoost\n",
      "4 = ANN\n",
      "5 = SVC\n",
      "6 = Voting\n",
      "ALL = alla modeller\n",
      "\n",
      "SMOTE-l√§gen:\n",
      "0 = Ingen SMOTE\n",
      "1 = Basic SMOTE (standardparametrar)\n",
      "2 = GRID-SMOTE (enkel tuning)\n",
      "3 = SMOTUNED-DE (evolution√§r tuning)\n",
      "4 = J√§mf√∂r ALLA SMOTE-varianter f√∂r vald dataset + modell\n",
      "JM1: shape=(13204, 22)\n",
      "Klassf√∂rdelning (hela datan):\n",
      "defects\n",
      "0    11101\n",
      "1     2103\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Train klassf√∂rdelning:\n",
      "defects\n",
      "0    8881\n",
      "1    1682\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test klassf√∂rdelning:\n",
      "defects\n",
      "0    2220\n",
      "1     421\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Efter basic SMOTE:\n",
      "defects\n",
      "0    8881\n",
      "1    8881\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "GRID-SMOTE ‚Äì b√§sta parametrar: {'smote__k_neighbors': 5, 'smote__sampling_strategy': 1.0}\n",
      "SMOTUNED-DE ‚Äì b√§sta parametrar:\n",
      "k_neighbors: 14\n",
      "sampling_strategy: 0.9998369460786775\n",
      "\n",
      "=== J√§mf√∂relse SMOTE-varianter ‚Äì dataset: JM1, modell: LogisticRegression ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>smote_mode</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JM1</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0.842484</td>\n",
       "      <td>0.532468</td>\n",
       "      <td>0.097387</td>\n",
       "      <td>0.164659</td>\n",
       "      <td>0.719646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JM1</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>BASIC</td>\n",
       "      <td>0.711094</td>\n",
       "      <td>0.291971</td>\n",
       "      <td>0.570071</td>\n",
       "      <td>0.386163</td>\n",
       "      <td>0.720185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JM1</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>GRID</td>\n",
       "      <td>0.711094</td>\n",
       "      <td>0.291971</td>\n",
       "      <td>0.570071</td>\n",
       "      <td>0.386163</td>\n",
       "      <td>0.720185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JM1</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>SMOTUNED-DE</td>\n",
       "      <td>0.709580</td>\n",
       "      <td>0.290049</td>\n",
       "      <td>0.567696</td>\n",
       "      <td>0.383936</td>\n",
       "      <td>0.719507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset               model   smote_mode  accuracy  precision    recall  \\\n",
       "0     JM1  LogisticRegression         NONE  0.842484   0.532468  0.097387   \n",
       "1     JM1  LogisticRegression        BASIC  0.711094   0.291971  0.570071   \n",
       "2     JM1  LogisticRegression         GRID  0.711094   0.291971  0.570071   \n",
       "3     JM1  LogisticRegression  SMOTUNED-DE  0.709580   0.290049  0.567696   \n",
       "\n",
       "         f1       auc  \n",
       "0  0.164659  0.719646  \n",
       "1  0.386163  0.720185  \n",
       "2  0.386163  0.720185  \n",
       "3  0.383936  0.719507  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 per SMOTE-l√§ge:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smote_mode</th>\n",
       "      <th>BASIC</th>\n",
       "      <th>GRID</th>\n",
       "      <th>NONE</th>\n",
       "      <th>SMOTUNED-DE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>JM1</th>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.386163</td>\n",
       "      <td>0.386163</td>\n",
       "      <td>0.164659</td>\n",
       "      <td>0.383936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "smote_mode                     BASIC      GRID      NONE  SMOTUNED-DE\n",
       "dataset model                                                        \n",
       "JM1     LogisticRegression  0.386163  0.386163  0.164659     0.383936"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Del 1: Imports & grundinst√§llningar ===\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.base import clone\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from scipy.optimize import differential_evolution\n",
    "from IPython.display import display\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Anpassa beroende p√• var din notebook ligger:\n",
    "# - om notebook ligger i Thesis/notebooks ‚Üí \".. / data\"\n",
    "# - om notebook ligger direkt i Thesis ‚Üí \"data\"\n",
    "DATA_DIR = os.path.join(\"..\", \"data\")\n",
    "# DATA_DIR = \"data\"  # anv√§nd denna ist√§llet om notebooken ligger i rotmappen\n",
    "\n",
    "\n",
    "# === Del 2: utv√§rderingsfunktion ===\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Tar en TR√ÑNAD modell + testdata och r√§knar nyckeltal.\n",
    "    Returnerar en dict med metrics + y_pred/y_proba.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        # t.ex. SVC utan predict_proba men med decision_function\n",
    "        y_proba_raw = model.decision_function(X_test)\n",
    "        # skala om till [0,1] om det beh√∂vs\n",
    "        y_proba = (y_proba_raw - y_proba_raw.min()) / (y_proba_raw.max() - y_proba_raw.min() + 1e-9)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"y_proba\": y_proba,\n",
    "    }\n",
    "\n",
    "\n",
    "# === Del 3: Dataset-konfiguration & f√∂rberedelse ===\n",
    "\n",
    "DATASETS = {\n",
    "    \"JM1\": {\"filename\": \"jm1.csv\", \"target\": \"defects\"},\n",
    "    \"KC1\": {\"filename\": \"kc1.csv\", \"target\": \"defects\"},\n",
    "    \"KC2\": {\"filename\": \"kc2.csv\", \"target\": \"defects\"},\n",
    "    \"PC1\": {\"filename\": \"pc1.csv\", \"target\": \"defects\"},\n",
    "    \"CM1\": {\"filename\": \"cm1.csv\", \"target\": \"defects\"},\n",
    "}\n",
    "\n",
    "def load_and_prepare_dataset(dataset_name):\n",
    "    \"\"\"\n",
    "    L√§ser in valt dataset, delar i train/test, skalar features.\n",
    "    SMOTE g√∂r vi separat beroende p√• menyval.\n",
    "    \"\"\"\n",
    "    info = DATASETS[dataset_name]\n",
    "    path = os.path.join(DATA_DIR, info[\"filename\"])\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    target_col = info[\"target\"]\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    # s√§kerst√§ll 0/1\n",
    "    if y.dtype == \"bool\":\n",
    "        y = y.astype(int)\n",
    "    elif y.dtype == \"object\":\n",
    "        y = y.str.lower().map({\"yes\": 1, \"true\": 1, \"defective\": 1}).fillna(0).astype(int)\n",
    "\n",
    "    print(f\"{dataset_name}: shape={df.shape}\")\n",
    "    print(\"Klassf√∂rdelning (hela datan):\")\n",
    "    print(y.value_counts(), \"\\n\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        stratify=y,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    print(\"Train klassf√∂rdelning:\")\n",
    "    print(y_train.value_counts())\n",
    "    print(\"\\nTest klassf√∂rdelning:\")\n",
    "    print(y_test.value_counts(), \"\\n\")\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "\n",
    "# === Del 4: Modeller (alla basmodeller) ===\n",
    "\n",
    "def get_base_models():\n",
    "    \"\"\"\n",
    "    Skapar alla modeller vi vill testa.\n",
    "    \"\"\"\n",
    "    log_reg = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    ann = MLPClassifier(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation=\"relu\",\n",
    "        max_iter=200,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    svc = SVC(\n",
    "        kernel=\"rbf\",\n",
    "        probability=True,   # beh√∂vs f√∂r AUC\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    voting = VotingClassifier(\n",
    "        estimators=[\n",
    "            (\"logreg\", log_reg),\n",
    "            (\"rf\", rf),\n",
    "            (\"xgb\", xgb),\n",
    "        ],\n",
    "        voting=\"soft\"  # anv√§nder sannolikheter\n",
    "    )\n",
    "\n",
    "    models = {\n",
    "        \"LogisticRegression\": log_reg,\n",
    "        \"RandomForest\": rf,\n",
    "        \"XGBoost\": xgb,\n",
    "        \"ANN\": ann,\n",
    "        \"SVC\": svc,\n",
    "        \"Voting\": voting,\n",
    "    }\n",
    "    return models\n",
    "\n",
    "\n",
    "# === Del 5: SMOTE-varianter ===\n",
    "\n",
    "def apply_basic_smote(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Standard-SMOTE med default-parametrar.\n",
    "    \"\"\"\n",
    "    smote = SMOTE(random_state=RANDOM_STATE)\n",
    "    X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "    print(\"Efter basic SMOTE:\")\n",
    "    print(pd.Series(y_res).value_counts(), \"\\n\")\n",
    "    return X_res, y_res\n",
    "\n",
    "\n",
    "def smote_grid_search(model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Enkel grid search p√• SMOTE-parametrar (inspirerad av SMOTUNED-id√©n).\n",
    "    \"\"\"\n",
    "    pipe = Pipeline([\n",
    "        (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
    "        (\"clf\", model),\n",
    "    ])\n",
    "\n",
    "    # OBS: dubbel underscore f√∂r pipeline-parametrar!\n",
    "    param_grid = {\n",
    "        \"smote__k_neighbors\": [3, 5, 7],\n",
    "        \"smote__sampling_strategy\": [0.5, 0.75, 1.0],\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        pipe,\n",
    "        param_grid,\n",
    "        scoring=\"f1\",\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(\"GRID-SMOTE ‚Äì b√§sta parametrar:\", grid.best_params_)\n",
    "    return grid.best_estimator_\n",
    "\n",
    "\n",
    "def smotuned_de(model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    F√∂renklad SMOTUNED-id√©:\n",
    "    differential evolution optimerar SMOTE-parametrar (k_neighbors, sampling_strategy)\n",
    "    f√∂r att maximera F1 med 3-fold CV.\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(params):\n",
    "        # params = [k_neighbors, sampling_strategy]\n",
    "        k = int(round(params[0]))\n",
    "        k = max(2, min(k, 15))   # h√•ll k inom [2, 15]\n",
    "\n",
    "        sampling = float(params[1])\n",
    "        sampling = max(0.2, min(sampling, 1.0))  # sampling_strategy inom [0.2, 1.0]\n",
    "\n",
    "        smote = SMOTE(\n",
    "            k_neighbors=k,\n",
    "            sampling_strategy=sampling,\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "\n",
    "        X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scores = []\n",
    "\n",
    "        for train_idx, val_idx in cv.split(X_res, y_res):\n",
    "            X_tr, X_val = X_res[train_idx], X_res[val_idx]\n",
    "            y_tr, y_val = y_res[train_idx], y_res[val_idx]\n",
    "\n",
    "            m = clone(model)\n",
    "            m.fit(X_tr, y_tr)\n",
    "            y_pred = m.predict(X_val)\n",
    "            scores.append(f1_score(y_val, y_pred, zero_division=0))\n",
    "\n",
    "        # differential_evolution minimerar, s√• vi returnerar -F1\n",
    "        return -np.mean(scores)\n",
    "\n",
    "    bounds = [\n",
    "        (2, 15),    # k_neighbors\n",
    "        (0.2, 1.0), # sampling_strategy\n",
    "    ]\n",
    "\n",
    "    result = differential_evolution(\n",
    "        objective,\n",
    "        bounds,\n",
    "        maxiter=15,\n",
    "        popsize=10,\n",
    "        tol=0.01,\n",
    "        polish=True,\n",
    "        disp=False,\n",
    "    )\n",
    "\n",
    "    best_k = int(round(result.x[0]))\n",
    "    best_sampling = float(result.x[1])\n",
    "    best_k = max(2, min(best_k, 15))\n",
    "    best_sampling = max(0.2, min(best_sampling, 1.0))\n",
    "\n",
    "    print(\"SMOTUNED-DE ‚Äì b√§sta parametrar:\")\n",
    "    print(\"k_neighbors:\", best_k)\n",
    "    print(\"sampling_strategy:\", best_sampling)\n",
    "\n",
    "    best_smote = SMOTE(\n",
    "        k_neighbors=best_k,\n",
    "        sampling_strategy=best_sampling,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "    X_res_best, y_res_best = best_smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    final_model = clone(model)\n",
    "    final_model.fit(X_res_best, y_res_best)\n",
    "\n",
    "    return final_model\n",
    "\n",
    "\n",
    "# === Del 6: j√§mf√∂relsefunktion f√∂r EN modell + EN dataset ===\n",
    "\n",
    "def compare_smote_variants(dataset_name, model_name):\n",
    "    \"\"\"\n",
    "    K√∂r SAMMA dataset + SAMMA modell med:\n",
    "    - ingen SMOTE\n",
    "    - basic SMOTE\n",
    "    - GRID-SMOTE\n",
    "    - SMOTUNED-DE\n",
    "    och returnerar en liten tabell med nyckeltal.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Ladda och skala data\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = load_and_prepare_dataset(dataset_name)\n",
    "\n",
    "    # 2) H√§mta vald basmodell\n",
    "    base_models = get_base_models()\n",
    "    if model_name not in base_models:\n",
    "        raise ValueError(f\"Modell '{model_name}' finns inte. Tillg√§ngliga: {list(base_models.keys())}\")\n",
    "    base_model = base_models[model_name]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # --- 0: Ingen SMOTE ---\n",
    "    m_none = clone(base_model)\n",
    "    m_none.fit(X_train_scaled, y_train)\n",
    "    res_none = evaluate_model(m_none, X_test_scaled, y_test)\n",
    "    results.append({\n",
    "        \"dataset\": dataset_name,\n",
    "        \"model\": model_name,\n",
    "        \"smote_mode\": \"NONE\",\n",
    "        \"accuracy\": res_none[\"accuracy\"],\n",
    "        \"precision\": res_none[\"precision\"],\n",
    "        \"recall\": res_none[\"recall\"],\n",
    "        \"f1\": res_none[\"f1\"],\n",
    "        \"auc\": res_none[\"auc\"],\n",
    "    })\n",
    "\n",
    "    # --- 1: Basic SMOTE ---\n",
    "    X_train_smote, y_train_smote = apply_basic_smote(X_train_scaled, y_train)\n",
    "    m_basic = clone(base_model)\n",
    "    m_basic.fit(X_train_smote, y_train_smote)\n",
    "    res_basic = evaluate_model(m_basic, X_test_scaled, y_test)\n",
    "    results.append({\n",
    "        \"dataset\": dataset_name,\n",
    "        \"model\": model_name,\n",
    "        \"smote_mode\": \"BASIC\",\n",
    "        \"accuracy\": res_basic[\"accuracy\"],\n",
    "        \"precision\": res_basic[\"precision\"],\n",
    "        \"recall\": res_basic[\"recall\"],\n",
    "        \"f1\": res_basic[\"f1\"],\n",
    "        \"auc\": res_basic[\"auc\"],\n",
    "    })\n",
    "\n",
    "    # --- 2: GRID-SMOTE ---\n",
    "    m_grid = smote_grid_search(base_model, X_train_scaled, y_train)\n",
    "    res_grid = evaluate_model(m_grid, X_test_scaled, y_test)\n",
    "    results.append({\n",
    "        \"dataset\": dataset_name,\n",
    "        \"model\": model_name,\n",
    "        \"smote_mode\": \"GRID\",\n",
    "        \"accuracy\": res_grid[\"accuracy\"],\n",
    "        \"precision\": res_grid[\"precision\"],\n",
    "        \"recall\": res_grid[\"recall\"],\n",
    "        \"f1\": res_grid[\"f1\"],\n",
    "        \"auc\": res_grid[\"auc\"],\n",
    "    })\n",
    "\n",
    "    # --- 3: SMOTUNED-DE ---\n",
    "    m_de = smotuned_de(base_model, X_train_scaled, y_train)\n",
    "    res_de = evaluate_model(m_de, X_test_scaled, y_test)\n",
    "    results.append({\n",
    "        \"dataset\": dataset_name,\n",
    "        \"model\": model_name,\n",
    "        \"smote_mode\": \"SMOTUNED-DE\",\n",
    "        \"accuracy\": res_de[\"accuracy\"],\n",
    "        \"precision\": res_de[\"precision\"],\n",
    "        \"recall\": res_de[\"recall\"],\n",
    "        \"f1\": res_de[\"f1\"],\n",
    "        \"auc\": res_de[\"auc\"],\n",
    "    })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    print(f\"\\n=== J√§mf√∂relse SMOTE-varianter ‚Äì dataset: {dataset_name}, modell: {model_name} ===\")\n",
    "    display(df)\n",
    "\n",
    "    # liten pivot p√• F1 f√∂r att se snabbt vilken som vinner\n",
    "    pivot_f1 = df.pivot_table(\n",
    "        index=[\"dataset\", \"model\"],\n",
    "        columns=\"smote_mode\",\n",
    "        values=\"f1\"\n",
    "    )\n",
    "    print(\"\\nF1 per SMOTE-l√§ge:\")\n",
    "    display(pivot_f1)\n",
    "\n",
    "    return df, pivot_f1\n",
    "\n",
    "\n",
    "# === Del 7: Meny f√∂r att k√∂ra experiment ===\n",
    "\n",
    "def run_experiments_menu():\n",
    "    # v√§lj dataset\n",
    "    print(\"Tillg√§ngliga dataset:\")\n",
    "    dataset_names = list(DATASETS.keys())\n",
    "    for idx, name in enumerate(dataset_names, start=1):\n",
    "        print(f\"{idx} = {name}\")\n",
    "    print(\"ALL = alla dataset\")\n",
    "\n",
    "    dataset_choice = input(\"V√§lj dataset (t.ex. 1, 2, 3 eller JM1/KC1/ALL): \").strip().upper()\n",
    "\n",
    "    all_datasets_selected = False\n",
    "\n",
    "    if dataset_choice == \"ALL\":\n",
    "        datasets_to_run = dataset_names\n",
    "        all_datasets_selected = True\n",
    "    elif dataset_choice.isdigit():\n",
    "        idx = int(dataset_choice) - 1\n",
    "        if 0 <= idx < len(dataset_names):\n",
    "            datasets_to_run = [dataset_names[idx]]\n",
    "        else:\n",
    "            print(\"Ogiltigt sifferval, anv√§nder f√∂rsta datasetet.\")\n",
    "            datasets_to_run = [dataset_names[0]]\n",
    "    else:\n",
    "        # anta att anv√§ndaren skrev namnet direkt, t.ex. JM1\n",
    "        if dataset_choice in DATASETS:\n",
    "            datasets_to_run = [dataset_choice]\n",
    "        else:\n",
    "            print(\"Ogiltigt namn, anv√§nder f√∂rsta datasetet.\")\n",
    "            datasets_to_run = [dataset_names[0]]\n",
    "\n",
    "    # v√§lj modell(er)\n",
    "    models = get_base_models()\n",
    "    print(\"\\nTillg√§ngliga modeller:\")\n",
    "    model_names = list(models.keys())\n",
    "    for idx, name in enumerate(model_names, start=1):\n",
    "        print(f\"{idx} = {name}\")\n",
    "    print(\"ALL = alla modeller\")\n",
    "\n",
    "    model_choice = input(\"V√§lj modell (t.ex. 1, 2 eller RandomForest/ALL): \").strip()\n",
    "\n",
    "    all_models_selected = False\n",
    "\n",
    "    if model_choice.upper() == \"ALL\":\n",
    "        model_names_to_run = model_names\n",
    "        all_models_selected = True\n",
    "    elif model_choice.isdigit():\n",
    "        idx = int(model_choice) - 1\n",
    "        if 0 <= idx < len(model_names):\n",
    "            model_names_to_run = [model_names[idx]]\n",
    "        else:\n",
    "            print(\"Ogiltigt sifferval, anv√§nder f√∂rsta modellen.\")\n",
    "            model_names_to_run = [model_names[0]]\n",
    "    else:\n",
    "        if model_choice in models:\n",
    "            model_names_to_run = [model_choice]\n",
    "        else:\n",
    "            print(\"Ogiltigt modellnamn, anv√§nder f√∂rsta modellen.\")\n",
    "            model_names_to_run = [model_names[0]]\n",
    "\n",
    "    # v√§lj SMOTE-l√§ge\n",
    "    print(\"\\nSMOTE-l√§gen:\")\n",
    "    print(\"0 = Ingen SMOTE\")\n",
    "    print(\"1 = Basic SMOTE (standardparametrar)\")\n",
    "    print(\"2 = GRID-SMOTE (enkel tuning)\")\n",
    "    print(\"3 = SMOTUNED-DE (evolution√§r tuning)\")\n",
    "    print(\"4 = J√§mf√∂r ALLA SMOTE-varianter f√∂r vald dataset + modell\")\n",
    "    smote_mode = input(\"V√§lj 0 / 1 / 2 / 3 / 4: \").strip()\n",
    "\n",
    "    # üî∏ Specialfall: smote_mode 4 = k√∂r compare_smote_variants f√∂r EN kombination\n",
    "    if smote_mode == \"4\":\n",
    "        if len(datasets_to_run) == 1 and len(model_names_to_run) == 1:\n",
    "            ds = datasets_to_run[0]\n",
    "            mn = model_names_to_run[0]\n",
    "            df_compare, pivot_compare = compare_smote_variants(ds, mn)\n",
    "            return df_compare\n",
    "        else:\n",
    "            print(\"\\n‚ö† SMOTE-l√§ge 4 kr√§ver att du v√§ljer EXAKT ett dataset och en modell (inte ALL).\")\n",
    "            print(\"Byter till l√§ge 1 (Basic SMOTE) ist√§llet.\\n\")\n",
    "            smote_mode = \"1\"\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for ds in datasets_to_run:\n",
    "        print(\"\\n==============================\")\n",
    "        print(f\"K√∂r dataset: {ds}\")\n",
    "        print(\"==============================\\n\")\n",
    "\n",
    "        X_train_scaled, X_test_scaled, y_train, y_test = load_and_prepare_dataset(ds)\n",
    "\n",
    "        for model_name in model_names_to_run:\n",
    "            base_models = get_base_models()  # nya instanser\n",
    "            model = base_models[model_name]\n",
    "\n",
    "            print(f\"\\n--- Modell: {model_name} ---\")\n",
    "\n",
    "            # v√§lj tr√§ningsstrategi beroende p√• smote_mode\n",
    "            if smote_mode == \"0\":\n",
    "                print(\"Ingen SMOTE anv√§nds.\\n\")\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                used_model = model\n",
    "                smote_label = \"NONE\"\n",
    "\n",
    "            elif smote_mode == \"1\":\n",
    "                X_train_smote, y_train_smote = apply_basic_smote(X_train_scaled, y_train)\n",
    "                model.fit(X_train_smote, y_train_smote)\n",
    "                used_model = model\n",
    "                smote_label = \"BASIC\"\n",
    "\n",
    "            elif smote_mode == \"2\":\n",
    "                used_model = smote_grid_search(model, X_train_scaled, y_train)\n",
    "                smote_label = \"GRID\"\n",
    "\n",
    "            elif smote_mode == \"3\":\n",
    "                used_model = smotuned_de(model, X_train_scaled, y_train)\n",
    "                smote_label = \"SMOTUNED-DE\"\n",
    "\n",
    "            else:\n",
    "                print(\"Ogiltigt SMOTE-val, anv√§nder ingen SMOTE.\")\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                used_model = model\n",
    "                smote_label = \"NONE\"\n",
    "\n",
    "            # utv√§rdera\n",
    "            eval_results = evaluate_model(used_model, X_test_scaled, y_test)\n",
    "            print(f\"Resultat ‚Äì {ds} ‚Äì {model_name} ‚Äì SMOTE-l√§ge {smote_label}\")\n",
    "            for k, v in eval_results.items():\n",
    "                if k in [\"y_pred\", \"y_proba\"]:\n",
    "                    continue\n",
    "                print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "            all_results.append({\n",
    "                \"dataset\": ds,\n",
    "                \"model\": model_name,\n",
    "                \"smote_mode\": smote_label,\n",
    "                \"accuracy\": eval_results[\"accuracy\"],\n",
    "                \"precision\": eval_results[\"precision\"],\n",
    "                \"recall\": eval_results[\"recall\"],\n",
    "                \"f1\": eval_results[\"f1\"],\n",
    "                \"auc\": eval_results[\"auc\"],\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(\"\\n=== Sammanfattning av alla k√∂rningar ===\")\n",
    "    display(results_df)\n",
    "\n",
    "    # Om du k√∂r ALL + ALL: visa pivot-tabell p√• F1\n",
    "    if all_datasets_selected and all_models_selected and not results_df.empty:\n",
    "        pivot_f1 = results_df.pivot_table(\n",
    "            index=[\"dataset\", \"model\"],\n",
    "            columns=\"smote_mode\",\n",
    "            values=\"f1\"\n",
    "        )\n",
    "        print(\"\\n=== F1 per dataset/modell och SMOTE-l√§ge ===\")\n",
    "        display(pivot_f1)\n",
    "\n",
    "    return results_df\n",
    "results_df = run_experiments_menu()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
